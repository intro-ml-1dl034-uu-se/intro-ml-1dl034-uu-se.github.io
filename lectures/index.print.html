<!doctype html><html lang=en dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=print><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.145.0"><meta name=generator content="Relearn 7.3.1"><meta name=description content="Lecture plan, timetable  and link to slides."><meta name=author content="Justin Pearson"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lectures :: 1DL034"><meta name=twitter:description content="Lecture plan, timetable  and link to slides."><meta property="og:url" content="https://intro-ml-1dl034-uu-se.github.io/lectures/index.html"><meta property="og:site_name" content="1DL034"><meta property="og:title" content="Lectures :: 1DL034"><meta property="og:description" content="Lecture plan, timetable  and link to slides."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Lectures :: 1DL034"><meta itemprop=description content="Lecture plan, timetable  and link to slides."><meta itemprop=wordCount content="438"><title>Lectures :: 1DL034</title>
<link href=https://intro-ml-1dl034-uu-se.github.io/lectures/index.html rel=canonical type=text/html title="Lectures :: 1DL034"><link href=/lectures/index.xml rel=alternate type=application/rss+xml title="Lectures :: 1DL034"><link href=/css/fontawesome-all.min.css?1740928677 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fontawesome-all.min.css?1740928677 rel=stylesheet></noscript><link href=/css/auto-complete.css?1740928677 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/auto-complete.css?1740928677 rel=stylesheet></noscript><link href=/css/perfect-scrollbar.min.css?1740928677 rel=stylesheet><link href=/css/theme.min.css?1740928677 rel=stylesheet><link href=/css/format-print.min.css?1740928677 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.relBasePath="..",window.relearn.relBaseUri="..",window.relearn.absBaseUri="https://intro-ml-1dl034-uu-se.github.io",window.relearn.min=`.min`,window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.themevariants=["blue"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}}))},window.relearn.markVariant=function(){var t=window.localStorage.getItem(window.relearn.absBaseUri+"/variant"),e=document.querySelector("#R-select-variant");e&&(e.value=t)},window.relearn.initVariant=function(){var e=window.localStorage.getItem(window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant(),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`</script></head><body class="mobile-support print" data-url=/lectures/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><button class=topbar-control onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/index.html><span itemprop=name>Introduction to Machine Learning 1DL034</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>Lectures</span><meta itemprop=position content="2"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-print" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><a class=topbar-control href=/lectures/index.print.html title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable lectures" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=lectures>Lectures</h1><p>Here you can find information on the lectures, as well as information
on when you should have enough information to be able to attempt the
various practical and theoretical python notebook assignments. The
project should be started as soon as possible, and you should try
different machine learning algorithms on the data set as you learn
them in the course.</p><p>There is a menu item for each lecture where you can find a reading
guide for the textbook, links to additional material and links to
slides.</p><p>Please note that the dates still could change, and the dates in
<a href=https://cloud.timeedit.net/uu/web/wr_staff/ri10hf577962ZcQY70Q9YQxXZQ0779Z0y5n088Yy4669lQngad_6bx1_86ttdl98xw95r4567pj8x2dy3B3QFZ1Q7bCal87ZC66981A9524896.phtml rel=external target=_blank>timeedit</a>
are always correct. If you find any errors, then please email me.</p><table><thead><tr><th>Lecture</th><th>Date</th><th>Topic</th><th></th></tr></thead><tbody><tr><td>1</td><td>21/1</td><td><a href=/lectures/lecture1/index.html>Introduction and Overview of the Course</a></td><td></td></tr><tr><td>2</td><td>22/1</td><td><a href=/lectures/lecture2/index.html>Linear Regression as Machine Learning</a></td><td></td></tr></tbody></table><p>You should now have all the knowledge you need to attempt practical
notebooks 1 and 2 (P1 & P2)</p><table><thead><tr><th>Lecture</th><th>Date</th><th>Topic</th><th></th></tr></thead><tbody><tr><td>Help Session</td><td>23/1</td><td></td><td></td></tr><tr><td>3</td><td>29/1</td><td><a href=/lectures/lecture3/index.html>Probability and Naive Bayes Classification</a></td><td></td></tr><tr><td>Help Session</td><td>30/1</td><td></td><td></td></tr></tbody></table><p>You now should have the knowledge to attempt the the first theoretical
notebook (T1).</p><table><thead><tr><th>Lecture</th><th>Date</th><th>Topic</th><th></th></tr></thead><tbody><tr><td>4</td><td>3/2</td><td><a href=/lectures/lecture4/index.html>Logistic Regression and Regularisation</a></td><td></td></tr></tbody></table><p>You should now be able to attempt the second theoretical notebook (T2)
and the third practical notebook (P3).</p><table><thead><tr><th>Lecture</th><th>Date</th><th>Topic</th><th></th></tr></thead><tbody><tr><td>5</td><td>6/2</td><td><a href=/lectures/lecture5/index.html>Support Vector Machines</a></td><td></td></tr><tr><td>Help Session</td><td>6/2</td><td></td><td></td></tr><tr><td>6</td><td>11/2</td><td><a href=/lectures/lecture6/index.html>Cross Validation and Feature Encoding</a></td><td></td></tr><tr><td>Help Session</td><td>13/2</td><td></td><td></td></tr></tbody></table><p>You should now be able to attempt the final practical notebook (P4).</p><table><thead><tr><th>Lecture</th><th>Date</th><th>Topic</th><th></th></tr></thead><tbody><tr><td>7</td><td>19/2</td><td><a href=/lectures/lecture7/index.html>Clustering and Nearest Neighbours</a></td><td></td></tr><tr><td>Help Session</td><td>20/2</td><td></td><td></td></tr><tr><td>8</td><td>21/2</td><td><a href=/lectures/lecture8/index.html>Decision Trees</a></td><td></td></tr></tbody></table><p>You should now be able to attempt theoretical notebook 3 (T3) and (T4)</p><table><thead><tr><th>Lecture</th><th>Date</th><th>Topic</th><th></th></tr></thead><tbody><tr><td>9</td><td>26/2</td><td><a href=/lectures/lecture9/index.html>Principle Component Analysis and Preprocessing</a></td><td></td></tr><tr><td>10</td><td>27/2</td><td><a href=/lectures/lecture10/index.html>Gradient Boosting & AdaBoost</a></td><td></td></tr><tr><td>11</td><td>3/3</td><td><a href=/lectures/lecture11/index.html>Ethics and Bias in Machine Learning</a></td><td></td></tr><tr><td>Exam</td><td>13/3</td><td></td><td></td></tr></tbody></table><h2 id=practical-and-theoretical-notebooks>Practical and Theoretical Notebooks</h2><p>There are 4 practical and theoretical notebooks that are done
individually as assignments. Links will be provided but the topics
covered in the notebooks are as follows.</p><h3 id=practical-notebooks>Practical Notebooks</h3><ul><li>P1 : Basic programming with Python, lists, sets and an introduction
to <a href=https://numpy.org/ rel=external target=_blank>NumPy</a>.</li><li>P2 : Introduction to <a href=https://pandas.pydata.org/ rel=external target=_blank>Pandas</a></li><li>P3 : Linear and Logistic regression with
<a href=https://scikit-learn.org/stable/ rel=external target=_blank>scikit-learn</a></li><li>P4 : Preprocessing, feature engineering and cross validation.</li></ul><h3 id=theoretical-notebooks>Theoretical Notebooks</h3><ul><li>T1 - Logistic Regression</li><li>T2 - Using naive Bayes for classifying tweets.</li><li>T3 - Using K-means classifiers</li><li>T4 - The Entropy of the normal distribution and binary decision
trees using ID3.</li></ul><h2 id=deadlines>Deadlines</h2><p>These should be the same deadlines as are in Studium. If there is any
discrepancy then please inform me.</p><table><thead><tr><th>What</th><th>When?</th></tr></thead><tbody><tr><td>P1 & P2</td><td>31/1 13:00</td></tr><tr><td>T1 & T2</td><td>10/2 13:00</td></tr><tr><td>P3 & P4</td><td>17/2 13:00</td></tr><tr><td>T3 & T4</td><td>24/2 13:00</td></tr><tr><td>Project</td><td>7/3 13:00</td></tr></tbody></table><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Lectures</h1><article class=default><header class=headline></header><h1 id=lecture-1-introduction-and-overview-of-the-course>Lecture 1: Introduction and Overview of the Course</h1><h2 id=todays-topic>Today&rsquo;s Topic</h2><p>Overview of the course and introduction to machine learning. What is
supervised learning and what is unsupervised learning.</p><h2 id=link-to-the-slides>Link to the slides.</h2><p>I use
<a href=https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture1.pdf rel=external target=_blank>these</a>
slides.</p><h2 id=reading-guide>Reading Guide</h2><ul><li><a href=http://themlbook.com/ rel=external target=_blank>The Hundred-Page Machine Learning Book</a> <a href="https://www.dropbox.com/s/lrhtt1wkffnm4fe/Chapter1.pdf?dl=0" rel=external target=_blank>Chapter
1</a>,
<a href="https://www.dropbox.com/s/0cprdghmnzpck8h/Chapter2.pdf?dl=0" rel=external target=_blank>Chapter
2</a>
contains some background mathematics and some definitions that you
should know.</li><li>Chapter 1 and 2 of <a href=https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview rel=external target=_blank>A Hands-On Introduction to Machine
Learning</a></li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>How is the course organised?</li><li>What is machine learning?</li><li>What is supervised learning?</li><li>What is unsupervised learning?</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=lecture-2-linear-regression-as-machine-learning>Lecture 2: Linear Regression as Machine Learning</h1><h2 id=todays-topics>Today&rsquo;s Topics</h2><h3 id=linear-regression>Linear Regression</h3><p>Linear Regression as a machine learning algorithm. Machine learning
algorithms and hypothesises. In short a machine learning find the best
hypothesis that explains that data. A cost function (or an error
function, or a loss function) measure how far way a hypothesis is from
explaining the data: the smaller the cost, the better the hypothesis.</p><p>Ideally you want an algorithm takes the training data and gives you
the hypothesis that with the smallest cost value. With linear
regression this is possible (using linear algebra), but in general it
is not possible.</p><p>If you have been reading about neural networks, then in a neural
network the weight roughly corresponds to the set of all possible
hypothesis.</p><h3 id=training-and-test-sets>Training and Test Sets</h3><p>As the course moves along we will learn more about best practices with
machine learning. The first important idea is that you should split
your data into a test set and a training set. If do not do this then
there is a possibility that you will over fit to your data set, and
when you meet new examples your machine learning system will not
perform that well. It is also important to keep in mind that when you
are using gradient descent to find the best hypothesis you use the
training set, but when you are evaluating the performance of the
learning algorithm you should use the test set. Later on when we look
at cross validation we will look at more advance ways to divide up
your data.</p><h2 id=links-to-slides>Links to Slides</h2><ul><li>The slides can be found
<a href=https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture2.pdf rel=external target=_blank>here.</a>.</li></ul><h2 id=reading-guide>Reading Guide</h2><ul><li><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a> <a href="https://www.dropbox.com/s/qiq2c85cle9ydb6/Chapter3.pdf?dl=0" rel=external target=_blank>Chapter
3</a>
section 3.1 and
<a href="https://www.dropbox.com/s/xpd5x6p6jte3th5/Chapter4.pdf?dl=0" rel=external target=_blank>Chapter
4</a>.</li><li>Chapter 4 of <a href=https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview rel=external target=_blank>A Hands-On Introduction to Machine
Learning</a></li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>How does linear regression work with one variable?</li><li>How does linear regression work with many variables?</li><li>What is a hypothesis in a machine learning algorithm?</li><li>What is a cost function? Note that in machine learning there is no
standard terminology. This is because machine learning comes from
many different disciplines. Other names for the cost function are
the Error function and the loss function.</li><li>What is the goal of a machine learning algorithm with the hypothesis
and the cost function? What does the cost function measure? Why is a
low value of the cost function desirable?</li><li>How does gradient descent work for linear regression? Can you derive
it from first principles?</li><li>Why is it necessary to split the data up into a training and a test
set?</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=lecture-3-probability-and-naive-bayes-classification>Lecture 3: Probability and Naive Bayes Classification</h1><h2 id=todays-topic>Today&rsquo;s Topic</h2><p>Using Bayes&rsquo; theorem for machine learning. You should do some revision
on the use of Bayes&rsquo; theorem in general. In this lecture you will look
at how to use Bayes&rsquo; theorem to build a spam detector. One important
idea to take away from this lecture is that there are a variety of
ways implementing spam detection: in particular there are different
feature models that you can use that give you different ways of
calculating the relevant probabilities. It is important that you
understand the difference between the different ways of implementing
spam detection.</p><h2 id=reading-guide>Reading Guide</h2><ul><li><p><a href=https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture3.pdf rel=external target=_blank>Lecture
Slides</a></p></li><li><p><a href="https://www.dropbox.com/s/0cprdghmnzpck8h/Chapter2.pdf?dl=0" rel=external target=_blank>Chapter
2</a>
of <a href=http://themlbook.com/ rel=external target=_blank>The Hundred-Page Machine Learning Book</a>
contains some background on probability and Bayes&rsquo; theorem.</p></li><li><p>My <a href=/lectures/lecture3/naive_bayes_spam/index.html>notes</a> on Naive Bayes for
spam detection talk about the different ways of calculating the
probabilities involved.</p></li><li><p>Section 6.4 of <a href=https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview rel=external target=_blank>A Hands-On Introduction to Machine
Learning</a></p></li><li><p><a href=https://courses.cs.washington.edu/courses/cse312/18sp/lectures/naive-bayes/naivebayesnotes.pdf rel=external target=_blank>Jonathan Lee&rsquo;s notes on Naive Bayes for Spam
Filtering</a>
notes are a bit more mathematical.</p></li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>How do I use Bayes&rsquo; theorem?</li><li>How do I use Bayes&rsquo; theorem to build a simple spam detector that
only uses one word?</li><li>What is independence assumption in the naive Bayes&rsquo; algorithm?</li><li>What are the various models estimating the probabilities for spam
detection?</li><li>What is Laplacian smoothing and how does it work in the context of
spam detection?</li><li>When do you need to use logarithms to calculate the relevant
probabilities, and how do you use them?</li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Lecture 3</h1><article class=default><header class=headline></header><h1 id=naive-bayes-for-spam-classification>Naive Bayes for Spam Classification</h1><p>There are a lot of tutorials and youtube videos out there on using
Naive Bayes for document classification. None of these tutorials are
wrong, but they often hide some subtle points that if you think too
hard about you will get confused. In this posts I want to explain what
is really going on in Naive Bayes for spam classification.</p><p>This post assumes that you are already familiar with <a href=https://en.wikipedia.org/wiki/Bayes%27_theorem rel=external target=_blank>Bayes&rsquo; theorem</a>.</p><p>Rather foolishly I did all the calculations in the post by hand. If
you find any errors the please report them to me.</p><h2 id=our-data-set>Our data set</h2><span class="math align-center">To make things more concrete we will work on a very small data set
where we can do the calculations directly. We are classifying
micro-tweets of exactly 3 words. Our training set is as follows. $S$
indicates that a message is spam and $\neg S$ indicates that a
message is not spam.
</span><span class="math align-center">$\neg S$</span><table><thead><tr><th>Number</th><th>Tweet</th><th>Spam (<span class="math align-center"> $S$ </span>or <span class="math align-center">$\neg S$ </span>)</th></tr></thead><tbody><tr><td>1</td><td>money aardvark boondoggle</td><td><span class="math align-center">$S$</span></td></tr><tr><td>2</td><td>money money money</td><td><span class="math align-center">$S$</span></td></tr><tr><td>3</td><td>money money world</td><td><span class="math align-center">$S$</span></td></tr><tr><td>4</td><td>money world world</td><td><span class="math align-center">$S$</span></td></tr><tr><td>5</td><td>viagra money back</td><td><span class="math align-center">$S$</span></td></tr><tr><td>6</td><td>viagra heart honey</td><td><span class="math align-center">$S$</span></td></tr><tr><td>7</td><td>aardvark boondoggle world</td><td><span class="math align-center">$\neg S$ </span>(not spam)</td></tr><tr><td>8</td><td>honey honey honey</td><td><span class="math align-center">$\neg S$ </span>(not spam)</td></tr><tr><td>9</td><td>viagra heart money</td><td><span class="math align-center">$\neg S$ </span>(not spam)</td></tr><tr><td>10</td><td>money honey now</td><td><span class="math align-center">$\neg S$ </span>(not spam)</td></tr></tbody></table><h2 id=background-classifying-with-only-one-word>Background: Classifying with only one word.</h2><span class="math align-center">As a warm up let's just build a classifier that uses one particular
word $w = \mathrm{money}$.
Bayes' Theorem should be familiar to you by now:
$$
P(S|w) = \frac{P(w|S)P(S)}{P(w)}
$$
$P(S|w)$ is the even that an email is Spam given that the word $w$
occurs in it. Using Bayes theorem we can calculate the probability
that a message containing the word $w$ is spam. We can estimate the
values $P(w|S)$, $P(S)$ and $P(w)$ from out data set.
$$
P(S) = \frac{\mathrm{number\ spam}}{\mathrm{total\ messages}} = \frac{6}{10}
$$
To estimate $P(w|S)$ we have to count the number of times that a
particular word occurs in a spam message. So
$$
P(\mathrm{money}|S) = \frac{5}{6}
$$
When you are only considering a single word then estimating $P(\mathrm{money})$
is easy. It is the ratio of the number of tweets that contain the
word 'money' and the total number of tweets. The word money appears in
$7$ tweets. So
$$
P(\mathrm{money}) = \frac{7}{10}
$$
So if we get a message that contains the word 'money' we can calculate
the probability that it is a spam message. $$ P(S|\mathrm{money}) =
\frac{P(w|S)P(S)}{P(w)} = \frac{\frac{5}{6}\frac{6}{10}}{\frac{7}{10}}
= \frac{5}{10}\frac{10}{7} = \frac{5}{7} \approx 0.71 $$ There is an
important identity that will become useful later on $$
P(\mathrm{money}) = P(\mathrm{money}|S)P(S) +
P(\mathrm{money}|\neg S)P(\neg S) $$ So $$
P(\mathrm{money})= \frac{5}{6}\frac{6}{10} + \frac{2}{4}\frac{4}{10} =
\frac{7}{10} $$</span><h2 id=first-pitfall-estimating-the-probabilities>First Pitfall: Estimating the probabilities</h2><span class="math align-center">So how to we estimate the probabilities $P(w|S)$ , $P(S)$ and $P(w)$?
What do they really mean? The probabilities $P(S)$ and $P(\neg S$) are
unambiguous. They are just the probability that a tweet is spam or
not. But $P(w|S)$, $P(w|\neg S)$, and $P(w)$ can mean different things
depending exactly which model we use to calculate the probabilities.</span><p>There are two models:</p><ul><li><p>(A): <span class="math align-center">To calculate $P(\mathrm{money}|S)$. There are 6
messages that are spam and in those 6 messages 5 of them (1,2,3,4,5)
contain the word money so $P(\mathrm{money}|S) = 5/6$, and of the 10
messages 7 of them (1,2,3,4,5,9,10) contain the word 'money' so
$P(\mathrm{money}) = 7/10$. This is exactly what we did above.</span></p></li><li><p>(B): <span class="math align-center">To calculate $P(\mathrm{money})$ there are $10\times 3 = 30$
words in our training set and the word money appears 10 times so
$$P(\mathrm{money}) = 10/30.$$ To calculate $P(\mathrm{money}|S)$
there are 6 spam messages each of 3 words long. In the words of the
spam messages the word 'money' appears 8 times. So $$
P(\mathrm{money} | S) = \frac{8}{3 \times 6} = \frac{8}{18} =
\frac{4}{9} $$
The probability a message being spam is still
$6/10$. So
if I get the message 'money for nothing' then the probability that
it is spam is calculated as before $$ P(S|\mathrm{money}) =
\frac{P(\mathrm{money}|S)P(S)}{P(\mathrm{money})} =
\frac{{\frac{8}{3 \times 6}\times \frac{6}{10}}}{ \frac{10}{30}} =
\frac{8}{10} $$
It seems that if spammers are prone to repeat words in their
message then this increases the probability that a message
containing that word is spam.</span></p></li></ul><span class="math align-center">So how do you calculate the probability that the message 'money money
mammon' is spam? In model (A) it does not matter how many times 'money'
appears in a message: you only count the number of messages 'money'
appears in. While in model (B) there is some weighting of the number
of times that a word appears. But to calculate
$P(S|\mathrm{money}^2)$ (where is short hand for 'money appearing
twice) we have calculate $P(\mathrm{money}^2|S)$. How you do this
depends a bit on your model and the assumptions underlying the
model. We'll get to that later.</span><details open class="box cstyle notices info"><summary class=box-label tabindex=-1><i class="fa-fw fas fa-info-circle"></i>
Take Home Message</summary><div class=box-content><h2 id=take-home-message>Take home message</h2><p>So the first take home message is be careful how you count the words
and how you calculate the probabilities. If you confuse model (A) and
model (B) while writing your code you will get strange answers (as I
did at one point).</p></div></details><h2 id=naive-bayes-first-version>Naive Bayes: first version</h2><p>We are going to use model (A). That is we going to ignore how many
times a word appears in a message. We are only interested if the word
appears on the message or not.
One word is not going to much of a spam classifier. Even in our little
data set above, the word &lsquo;money&rsquo; can appear in spam and non spam
messages. We will get a better classifier if we take into account more
words. Our data set is quite small and for each word we can count the
number of times it appears in a spam tweet and the number of times it
appears in a non-spam tweet.</p><table><thead><tr><th>Word</th><th>occurrences in spam</th><th>occurrences in non spam</th></tr></thead><tbody><tr><td><span class="math align-center">$w_1 = \mathrm{money}$</span></td><td>5</td><td>2</td></tr><tr><td><span class="math align-center">$w_2 = \mathrm{world}$</span></td><td>2</td><td>1</td></tr><tr><td><span class="math align-center">$w_3 = \mathrm{viagra}$</span></td><td>2</td><td>1</td></tr><tr><td><span class="math align-center">$w_4 = \mathrm{aardvark}$</span></td><td>1</td><td>1</td></tr><tr><td><span class="math align-center">$w_5 = \mathrm{heart}$</span></td><td>1</td><td>1</td></tr><tr><td><span class="math align-center">$w_6 = \mathrm{boondoggle}$</span></td><td>1</td><td>1</td></tr><tr><td><span class="math align-center">$w_7 = \mathrm{honey}$</span></td><td>1</td><td>2</td></tr><tr><td><span class="math align-center">$w_8 = \mathrm{back}$</span></td><td>1</td><td>0</td></tr><tr><td><span class="math align-center">$w_9 = \mathrm{now}$</span></td><td>0</td><td>1</td></tr></tbody></table><span class="math align-center">You can turn these counts into probabilities, and thus you can
calculate quantities like $P(\mathrm{money}|S) = 5/6$,
$P(\mathrm{money}|\neg S) = 2/4$.
Suppose I receive a message 'viagra money boondoggle' what is the
probability that it is spam message? When we use Bayes' theorem we
have to calculate $$P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle}|S)$$ where $$\mathrm{viagra} \land \mathrm{money}
\land \mathrm{boondoggle}$$ is the event that the words 'viagra',
'money' and 'boondoggle' appears in a message.</span><h2 id=the-naive-in-naive-bayes>The Naive in Naive Bayes</h2><span class="math align-center">We need to make an independence assumption. In a spam or non spam
message the probability of words are independent. That is
$$
P(w_1 \land w_2 \land \cdots \land w_n | S) = P(w_1|S)P(w_1|S) \cdots
P(w_n|S)
$$
and
$$
P(w_1 \land w_2 \land \cdots \land w_n | \neg S) = P(w_1|\neg S)P(w_1|\neg S) \cdots
P(w_n|\neg S)
$$
Note this is a weaker assumption than simply saying
$$
P(w_1 \cdots w_n) = \prod_{1\leq i \leq n} P(w_i)
$$</span><h2 id=take-home-message>Take home message</h2><span class="math align-center">Note that because we have made the assumptions that
$$P(w_1 \land w_2
\land \cdots \land w_n | S) =
\prod_{i=1}^n P(w_i|S)$$
and
$$P(w_1 \land w_2 \land \cdots \land w_n | S) =
\prod_{i=1}^n P(w_i|S)$$
it does not make sense to directly estimate the probabilities of
$P(w_i)$ directly from the data set. Later on we will see that you do
actually need the probabilities $P(w_1 \cdots w_n)$ to decide if a
message is spam or not. If you want to calculate the probability
$P(w_1 \cdots w_n)$ then you must use the identity $P(w_1 \cdots
w_n)$ equals
$$
P(w_1 \cdots w_n|S)P(S) + P(w_1 \cdots w_n|\neg S)P(\neg S)
$$</span><p>The independence assumption is why Naive Bayes is referred to as
naive. Although this model could be improved. It seems that the
probability of one word appearing in a message should not be
independent of another word. If a spammer write &lsquo;money&rsquo; then he is
likely to also include &lsquo;viagra&rsquo; in the message. Even so, assuming
independence works very well in practice.</p><h2 id=calculating-the-spam-probability>Calculating the spam probability.</h2><span class="math align-center">We can now apply Bayes' theorem $P(S | \mathrm{viagra} \land
\mathrm{money} \land \mathrm{boondoggle})$ equals $$ \frac{
P(\mathrm{viagra} \land \mathrm{money} \land \mathrm{boondoggle}|S)
P(S)}{P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle})} $$
From the independence assumption we have that $P(S | \mathrm{viagra}
\land \mathrm{money} \land \mathrm{boondoggle})$ equals $$ \frac{
P(\mathrm{viagra}|S)P(\mathrm{money}|S)P(\mathrm{boondoggle}|S)P(S)}
{P(\mathrm{viagra} \land \mathrm{money} \land \mathrm{boondoggle})} $$
To calculate $P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle})$ we use the identity above.
Taking product $$P(\mathrm{viagra})P(\mathrm{money})P(\mathrm{boondoggle})$$
is the wrong answer.
So instead we get that $P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle})$ equals
$P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle}|S)P(S)$ plus $P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle}|\neg S)P(\neg S)$.
The by the independence assumption
$P(\mathrm{viagra} \land \mathrm{money} \land \mathrm{boondoggle}|S)$ equals
$$
P(\mathrm{viagra}|S)P(\mathrm{money}|S)P(\mathrm{boondoggle}|S) =
\frac{2}{6}\frac{5}{6}\frac{1}{6} = \frac{5}{108}
$$
Putting the numbers in we get $P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle})$ equals
$$ \left(\frac{2}{6}\cdot \frac{5}{6}\cdot
\frac{1}{6}\right)\frac{6}{10} + \left(\frac{1}{4}\cdot
\frac{2}{4}\cdot \frac{1}{4}\right)\frac{4}{10} \approx 0.08
$$
So $P(S|\mathrm{viagra} \land
\mathrm{money} \land \mathrm{boondoggle})$ equals
$$
\frac{\frac{5}{108}\frac{6}{10}}{0.08} \approx 0.35
$$</span><h2 id=not-calculating-the-whole-probability>Not calculating the whole probability.</h2><span class="math align-center">When implementing the spam filter we do not actually need to calculate
the denominators. We just compare the expressions
$P(\mathrm{viagra}|S)P(\mathrm{money}|S)P(\mathrm{boondoggle}|S)P(S)$
and
$P(\mathrm{viagra}|\neg S)P(\mathrm{money}|\neg S)P(\mathrm{boondoggle}|\neg S)P(\neg S)$
and see which one is bigger. This is also important because some of
the numbers start getting smaller and smaller and you end up with
floating point underflow errors. If the numbers get too small then you
have to calculate with the logarithm of the probability and do
additions rather than multiplication.</span><h2 id=laplacian-smoothing>Laplacian Smoothing</h2><span class="math align-center">What if we have the message 'now money viagra', if we look at our data
set the word 'now' has not appeared in a spam message. There could be
two reasons for this, one is that a spam message will never contain
the word 'now' (unlikely), or that we just do not have a spam message
with 'now' appearing in our training set. If we use model (A) and
calculate the probability that our message is spam we get
$P(S|\mathrm{now}\land\mathrm{money}\land{viagra})$ equals
$$
\frac{P(\mathrm{now}|S)P(\mathrm{money}|S)P(\mathrm{viagra}|S)P(S)}{P(\mathrm{now}\land\mathrm{money}\land\mathrm{viagra})}
$$
which equals
$$\frac{0 \cdot \frac{5}{6} \cdot
\frac{2}{6}\frac{6}{10}}{P(\mathrm{now}\land\mathrm{money}\land\mathrm{viagra})}
$$
So even though the words 'money' and 'viagra' are pretty good
indicators of a message being spam we get probability 0.
To get around this we add one to all our counts to avoid probability
$0$ estimates and adjust the total count so as to avoid any
probabilities greater than $1$. So in model (A) if we are considering
$9$ words as a above then we estimate $P(\mathrm{now}|S)$ to be
$$
\frac{0 + 1}{6 + 1}
$$
instead of
$$
\frac{0}{6}
$$
If you had a word that appeared in all 6 of the spam tweets then you
would get an estimate $\frac{6+1}{6+1}$ which would be $1$.
I leave it an exercise to work out the correct thing to do in model
(B).</span><h2 id=feature-modelling>Feature Modelling</h2><span class="math align-center">All most all machine learning algorithms require numbers and vectors as
inputs. Our Naive Bayes classifier does not really work with words,
but feature vectors. There are different possible models, but we use
something similar to model (A). First we take our data set and find
the first $n$ most popular words. The most popular word a data set
consisting of English messages is typically the word 'the'. You can
improve things by filtering out the most popular words that don't
contribute much a message being (referred to as stop words). We will
not worry about that here. But a word like 'the' is equally likely to
appear in a spam tweet or a non spam tweet, so it is better to ignore
it.
Then we turn each message into a vector of length $n$ where each entry
is $1$ or $0$, and the $i$th entry is $1$ if the message contains the
$i$th most popular word. So in a typical English message data set
'the' is the most popular and 'to' is the second most popular word. So
if our message contained the words 'to the' then the first two entries
of its feature vector would have the value $1$. $$ f = (f_1, \ldots ,
f_i, \ldots, f_n) $$ Where $f_i$ is $1$ if the $i$th most popular word
$w_i$ occurs in the message and $0$ otherwise.
It is easy to write a function takes a message and turns it into our
feature vector. Given our training set we can estimate two
probabilities for our two classes <span class="math align-center">$S$ </span>and <span class="math align-center">$\neg S$ </span>, $P(w_i|S)$, $P(\overline{w}_i|S)$, $P(w_i| \neg S)$ and $P(\overline{w}_i |
\neg S)$, where $w_i$ is the even that word $i$ occurs in the
message and $\overline{w}_i$ is the event that word $i$ does not occur
in the message.
In our example above we only have $9$ words in our data set and they
appear in order of popularity as 'money', 'world', 'viagra',
'aardvark', 'heart', 'boondoogle' , 'honey', 'back' , 'now'. You have
to break ties (words that are equally popular) and you have to do it
consistently. So give the message 'aardvark money now' its feature
vector would be $$ f = (1,0,0,1,0,0,0,0,1) $$
This vector $f$ corresponds to the event
$$
w_1\overline{w}_2\overline{w}_3w_4\overline{w}_5\overline{w}_6\overline{w}_7w_8
$$
So to use Bayes' theorem to work on the probability that the tweet
is is spam we have to
calculate the quantity
$$
\frac{P(w_1\overline{w}_2\overline{w}_3w_4\overline{w}_5\overline{w}_6\overline{w}_7\overline{w}_8|S)P(S)}{P(w_1\overline{w}_2\overline{w}_3w_4\overline{w}_5\overline{w}_6\overline{w}_7\overline{w}_8
w_9)}
$$
Calculating
$P(w_1\overline{w}_2\overline{w}_3w_4\overline{w}_5\overline{w}_6\overline{w}_7\overline{w}_8|S)$
is easily done by the independence assumption. It is the product of
the terms
$P(w_1|S)$,$P(\overline{w}_2|S)$,$P(\overline{w}_3|S)$,$P(w_4|S)$,$P(\overline{w}_5|S)$,$P(\overline{w}_6|S)$,$P(\overline{w}_7|S)$
and $P(\overline{w}_8|S)$. All these values are easily estimated from
our data set. For example $P(\overline{w}_3|S)$ is the probability
that the word 'money' does not appear in a spam tweet. We had 6 spam
tweets and 5 of them contained the word money and so we get that
$P(\overline{w}_3|S)$ equals $1/6$.</span><h2 id=model-a-and-feature-vectors>Model (A) and Feature Vectors</h2><span class="math align-center">If you go back to model (A) and you try to estimate if a message is
spam or not, then using the same message we would only need to
calculate
$$
\frac{P(w_1|S)(w_4|S)P(w_9|S)P(S)}{P(w_1\land w_w \land w_3)}
$$
since $w_1$ is 'money', $w_4$ is 'aardvark' and $w_9$ is 'now'. We
are throwing away information about the words that do not occur in the
tweet. Does this matter? More importantly is this calculation
incorrect.
To simply things imagine that we only had two words in our feature
vector $W_1$ and $W_2$. Then given a message there are 4 possible
atomic events:
$$ W_1 W_2, W_1 \overline{W}_2, \overline{W}_1 W_2, \overline{W}_1
\overline{W_2}$$
What do we mean when we write $P(W_1|S)$? Looking at our atomic
events we actually mean
$$
P( W_1 W_2 \lor W_1\overline{W}_2|S)
$$
Any event is a union of the atomic events in your probability model.
Using the independence assumption for $W_1$ and $W_2$ and the basic
rule of probability that $P(A\lor B)$ equals $P(A)+P(B)$ when then
events $A$ and $B$ are independent atomic events we get that
$P(W_1|S)$ equals $P( W_1 W_2 \lor W_1\overline{W}_2|S)$ which equals
$$
P(W_1|S)P(W_2|S) +
P(W_1|S)P(\overline{W}_2|S)$$ refactoring gives
$$P(W_1|S)(P(W_2|S) +
P(\overline{W}_2|S))
$$
and since $P(W_2|S) + P(\overline{W}_2|S)$ equals $1$ we get
$P(W_1|S)$.
So if we ignore any information about $W_2$ then we get a factor of
$1$. If we use what information we have about $W_2$ then we get a
better estimate for the probability. You can show that the same
applies if you have lots of words in your feature vector. So our
original model (A) is not wrong, but the feature vector model where we
take into account if a word appears or not when we are estimating the
probabilities and gives a better estimate if the word is spam or
not. Note the above argument depends on our independence assumption
(the naive in Naive Bayes).
If you did not look at any words then the only information that you
would have is $P(S)$ or $P(\neg S)$ as you look at more words in
your feature you get a better estimate of the probability that the
message is spam or not.</span><h2 id=model-b-with--multiple-words>Model (B) with multiple words</h2><span class="math align-center">How do you calculate the probability that the message 'money money
boondoggle' is spam? We have already assumed that the probabilities of
a words occurring in a spam or non-spam tweet are independent. If we
also assume that the probability of a word appearing $k$ times is
$$
P(w^k|S) = P(w|S)^k
$$
That is each occurrence is independent, then we can calculate the
probability that a message containing the multiple occurrences of a
word is spam or not, but only if you use model (B) to calculate the
probabilities. You should not mix up model (A) and model (B). It does
not make sense in model (A) to ask what the probability of a word
occurring $k$ times in a spam message is. We can only ask if a m
message contains the word or not.<details open class="box cstyle notices info"><summary class=box-label tabindex=-1><i class="fa-fw fas fa-info-circle"></i>
Take Home Message</summary><div class=box-content><p>If you want to take into account multiple words then do not use model
(A) to calculate your probabilities.</p></div></details></span><h2 id=model-b-with-multiple-words-and-feature-vectors>Model (B) with multiple words and feature vectors.</h2><span class="math align-center">The feature vector approach for model (A) considered vectors where the
entries are $0$ or $1$. Given a feature vector $f$ the entry $i$th
entry is $1$ if the word appears in the message and $0$ otherwise.
Instead we would have a feature vector where the $ith$ entry tells you
how many times the word appears in the message. Thus for our message 'money money
boondoggle' its feature vector would be (using the same ordering as
above):
$$
(2,0,0,0,0,1,0,0,0)
$$
Again it is not hard to use the information that a word appears $0$
times.</span><h2 id=take-home-messages>Take home messages</h2><ul><li><p>Are you using model (A) or model (B). Don&rsquo;t get them confused,
especially when you are estimating the probabilities <span class="math align-center">$P(w_i|S)$</span> from
the training data.</p></li><li><p>Are you using the negative information, the information that a word
does not occur? It does not matter your maths is correct, but you
are not using all the information that you have.</p></li><li><p>To understand how this is related to other machine learning
algorithms, then you have to understand that we take a message and
construct a feature vector. Depending on if you are using model (A)
or (B) your feature vector either has <span class="math align-center">$0$</span> or <span class="math align-center">$1$</span> entries or positive
integer that tells you how many times a word occurs in a
message. Feature vectors is an example modelling that you often have
to do in machine learning. Your data set and package does not
always come as ready packaged as a set of vectors.</p></li><li><p>If you watch some random video on the internet, it is not always
clear which model they are using when they calculate the
probabilities.</p></li></ul><p>The documentation to <a href=https://scikit-learn.org/stable/ rel=external target=_blank>scikit-learn</a>
has a nice entry on <a href=https://scikit-learn.org/stable/modules/naive_bayes.html rel=external target=_blank>naive
Bayes</a>, which
discusses the various options on modelling as well as links to various
interesting articles on the different modelling approaches to naive
Bayes.</p><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=lecture-4-logistic-regression-as-machine-learning>Lecture 4: Logistic Regression as Machine Learning</h1><h2 id=todays-topics>Today&rsquo;s Topics</h2><p>Today&rsquo;s
<a href=https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture4.pdf rel=external target=_blank>slides</a>.</p><h3 id=logistic-regression>Logistic Regression</h3><p>Logistic regression, overfitting and regularisation. Again Logistic
regression is an algorithm that comes from statistics, but it can also
seen as a machine learning algorithm. The hypothesis is very similar
to linear regression is it a set of values that defines a linear
function. The difference between logistic regression and linear
regression is the linear function goes through a logistic function
that works as a threshold function. Unlike linear regression it is
not possible to solve the model exactly, and gradient descent is
necessary.</p><p>There are a lot of ways of
thinking about how logistic regression works.</p><ul><li>As a modification of linear regression to get $0$ or $1$ values to
divide the data set into two halves, or two find a separating
hyperplane between the two classes.</li><li>As an estimator of the probability that point begins to one class
or another.</li><li>As a single neuron. You can see logistic regression as the beginning
of neural networks.</li></ul><h3 id=overfitting-and-regularisation>Overfitting and Regularisation</h3><p>Both linear and logistic regression can be improved with a
regularisation term that avoids overfitting. You should try to begin
to understand why overfitting is a problem and some strategies for
avoiding it.</p><h2 id=reading-guide>Reading Guide</h2><h3 id=logistic-regression-1>Logistic Regression</h3><ul><li><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a> <a href="https://www.dropbox.com/s/qiq2c85cle9ydb6/Chapter3.pdf?dl=0" rel=external target=_blank>Chapter
3</a>
section 3.2.</li><li>6.1 and 6.2 of <a href=https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview rel=external target=_blank>A Hands-On Introduction to Machine
Learning</a></li></ul><h3 id=overfitting-and-regularisation-1>Overfitting and Regularisation</h3><ul><li><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a> <a href="https://www.dropbox.com/s/qiq2c85cle9ydb6/Chapter3.pdf?dl=0" rel=external target=_blank>Chapter
3</a>
section 3.1.2 and <a href="https://www.dropbox.com/s/nije38rerpfa18o/Chapter5.pdf?dl=0" rel=external target=_blank>Chapter 5</a> sections 5.4 and 5.5.</li></ul><h3 id=multiclass-classification>Multiclass classification.</h3><ul><li><a href=https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/ rel=external target=_blank>One-vs-Rest and
One-vs-One</a>
an excellent article by <a href=https://machinelearningmastery.com/about/ rel=external target=_blank>Jason Brownlee</a>.</li></ul><h3 id=confusion-matrices>Confusion Matrices</h3><ul><li>Again the <a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning
Book</a> <a href="https://www.dropbox.com/s/nije38rerpfa18o/Chapter5.pdf?dl=0" rel=external target=_blank>Chapter
5</a>
section 5.6 (but not 5.6.5 or 5.6.4).</li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>What is logistic regression and how does it differ from linear
regression?</li><li>What is the cost function? What does the logistic function do?</li><li>How do I implement gradient descent for logistic regression?</li><li>How does logistic regression relate to log-odds and what is it
relationship with probability.</li><li>What is overfitting?</li><li>How does the regularisation term work in linear and logistic
regression and how does it avoid overfitting.</li><li>How do you use a binary classifier for multi-class classification?
What is one-vs-all classification?</li><li>What is a confusion matrix?</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=lecture-5-support-vector-machines>Lecture 5: Support Vector Machines</h1><h2 id=todays-topic-support-vector-machines-svm>Today&rsquo;s topic Support Vector Machines (SVM)</h2><p>Support vector machines (SVMs) are used for classification, and use
some of the ideas from logistic regression. Support vector machines
deal with noisy data, where some labels are miss-classified, with
<em>large-margin classifiers</em>.</p><p>Support vector machines can also do non-linear classification using
<em>kernels</em>. A kernel is a non-linear transformation of the input data
into a higher dimensional space. Kernels transform your input data
into a space where it is possible to do linear separation as in
logistic regression.</p><p>This sounds very abstract, but the basic mathematics is not very
complicated. Support vector machines are very powerful, and before
you try neural networks try SVMs with different kernels.</p><p>These are the
<a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture5.pdf rel=external target=_blank>slides</a>
that I use.</p><h2 id=reading-guide>Reading Guide</h2><ul><li><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a> <a href="https://www.dropbox.com/s/qiq2c85cle9ydb6/Chapter3.pdf?dl=0" rel=external target=_blank>Chapter
3</a>
section 3.4, <a href="https://www.dropbox.com/s/lrhtt1wkffnm4fe/Chapter1.pdf?dl=0" rel=external target=_blank>Chapter
1</a>
sections 1.3 and 1.4 again and <a href="https://www.dropbox.com/s/esprbgjm0wc5afz/Chapter7.pdf?dl=0" rel=external target=_blank>Chapter
7</a>
sections 7.1,7.2 and 7.3.</li><li>The Wikipedia page on the <a href=https://en.wikipedia.org/wiki/Kernel_method rel=external target=_blank>Kernel
Method</a> is a good
starting point for the kernel trick and a reference on different
Kernels. The whole Wikipedia article on
<a href=https://en.wikipedia.org/wiki/Support-vector_machine rel=external target=_blank>SVMs</a> is also
a good starting point that includes a very good set of references.</li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>What are Support Vector Machines?</li><li>What is a large margin classifier?</li><li>What is the hinge loss function?</li><li>What are Kernels? You should know some common kernels including the
polynomial kernel and the Gaussian kernel (or radial bases kernel).</li><li>What is the Kernel trick?</li><li>How does the learning algorithm work for SVMs?</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=lecture-6-cross-validation-and-feature-engineering>Lecture 6: Cross Validation and Feature Engineering</h1><h2 id=todays-topics>Today&rsquo;s Topics</h2><p>In today&rsquo;s lecture you will look at various techniques to deal with
and understand overfitting. Dealing with overfitting leads nicely to
the model selection problem. First and foremost, how do you decide
which machine learning algorithm to use. Further, in many machine
learning algorithms there are hyper-parameters that are not decided by
the training data. Choosing which model to use or values of the models
hyper-parameters is a difficult task and can greatly affect the
performance of you algorithm. Cross validation is a useful technique
from statistics that allows you to partition your data up into many
combinations of training, test and validation sets. You can then use
cross validation to help you decide which machine learning model to
use and what values to set the hyper-parameters.</p><p>Finally we will cover various techniques for data-normalisation, and
dealing with categorical data. One important thing to remember is that
a little bit of data prepossessing can often improve the performance
of your learning algorithm significantly.</p><h2 id=slides>Slides</h2><p>I used these
<a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture6.pdf rel=external target=_blank>slides</a>
in the lecture.</p><h2 id=reading-guide>Reading Guide</h2><ul><li><p><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a> the
whole of <a href="https://www.dropbox.com/s/nije38rerpfa18o/Chapter5.pdf?dl=0" rel=external target=_blank>Chapter 5</a>.</p></li><li><p><a href=https://www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html rel=external target=_blank>Notes from Andrew Ng&rsquo;s Lectures</a></p></li><li><p><a href=https://machinelearningmastery.com/k-fold-cross-validation/ rel=external target=_blank>Machine learning mastery on cross
validation</a></p></li><li><p>Last year when I taught the course live, I used these
<a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture_cross.pdf rel=external target=_blank>notes</a>
that you might find useful.</p></li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>What is over fitting? what are some strategies to avoid it?</li><li>What is the bias</li><li>What is the model selection problem?</li><li>What are hyper-parameters?</li><li>Why do you need to split data in training, test and validation sets?</li><li>What is cross validation?</li><li>What is k-fold cross validation?</li><li>What are the different encoding strategies for categorical data?
One-hot encoding, dummy encoding?</li><li>What is data normalisation and when is it necessary?</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=lecture-7-clustering-and-nearest-neighbours>Lecture 7: Clustering and Nearest Neighbours</h1><h2 id=todays-topics>Today&rsquo;s Topics</h2><p>In this segment you are looking at some unsupervised algorithms, as
well as one supervised learning method (K-nearest neighbours) The
main unsupervised algorithms are Hierarchical clustering, K-means
clustering and DBSCAN. It is important not to get K-nearest neighbours
and K-means clustering confused.</p><p>The K-means algorithm works by gradient descent. Unlike a lot of the
algorithms that we have been looking at, K-means often suffers from
the problem of many local minima. In Andrew Ng&rsquo;s lectures you will
meet various ways of dealing with local minima.</p><p>If you have taken <a href=https://ad2-uu-se.github.io/ rel=external target=_blank>Algorithms and Data Structures II</a>(<a href="https://www.uu.se/en/admissions/master/selma/kursplan/?kKod=1DL231" rel=external target=_blank>1DL231</a>)
or AD3
(<a href="https://www.uu.se/en/admissions/master/selma/kursplan/?kKod=1DL481" rel=external target=_blank>1DL481</a>),
then you will have met the concept of <a href=https://ad2-uu-se.github.io/lectures/lectures12-13/index.html rel=external target=_blank>NP-hardness</a>. K-means clustering is NP-hard
(see the reference below). This means that the problem is not easy to
solve. If you could guarantee that there would only be one global
minimum then gradient descent would be an efficient algorithm. This
implies that there will always often be local minima in K-means
clustering.</p><h2 id=slides>Slides</h2><p>I used these
<a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture7.pdf rel=external target=_blank>slides</a>
in the lecture.</p><h2 id=reading-guide>Reading Guide</h2><h3 id=k-means-clustering>K-Means Clustering</h3><ul><li><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a> <a href="https://www.dropbox.com/s/qiq2c85cle9ydb6/Chapter3.pdf?dl=0" rel=external target=_blank>Chapter
3</a>
section 3.5 and <a href="https://www.dropbox.com/s/y9a7b0hzmuksqar/Chapter9.pdf?dl=0" rel=external target=_blank>Chapter
9</a> all
of 9.2</li><li>Chapter 7 of <a href=https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview rel=external target=_blank>A Hands-On Introduction to Machine
Learning</a>.</li><li>Chapter 6 (6.1 and 6.2) of <a href=https://tinyurl.com/y2obtldw rel=external target=_blank>A First Course in Machine
Learning</a>. The link takes you to the
electronic copy in the library.</li><li>This is only for reference: <a href=https://cseweb.ucsd.edu/~avattani/papers/kmeans_hardness.pdf rel=external target=_blank>NP Hardness of K-means
clustering</a>. You
don&rsquo;t need to understand the proof, although you should be aware of
its implications. No greedy/gradient descent algorithm for K-means
is going to be exact.</li></ul><h3 id=k-nearest-neighbours>K-Nearest Neighbours</h3><ul><li>The <a href=https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm rel=external target=_blank>Wikipedia page on K-Nearest
Neighbours</a>
is a good starting point.</li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>What are some of the applications of clustering?</li><li>What is hierarchical clustering and what algorithms are there?</li><li>How does the K-means algorithm work? What is the cost function?</li><li>What is a local optima and why is it a problem with the K-means
algorithm?</li><li>What are some approaches to choosing the number of clusters in
K-means?</li><li>How does the K-nearest neighbour algorithm work and what are some of
its applications?</li><li>What is DBSCAN and how does it work?</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=lecture-8-decision-trees>Lecture 8: Decision Trees</h1><h2 id=todays-topics>Today&rsquo;s Topics</h2><p>Decision trees are another powerful machine learning technique, and
has the advantage that it is easy to see what the algorithm has
learned. Constructing the optimal decision tree for an arbitrary data
set is yet another NP-hard question, and there are many algorithms
available. In this lecture you will look at one of the simpler
algorithm ID3. ID3 uses information theory to decide how to construct
a tree.</p><h2 id=slides-and-notebooks>Slides and Notebooks</h2><ul><li>Lecture
<a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture8.pdf rel=external target=_blank>slides</a></li><li><a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Notebooks/pandas_id3.ipynb rel=external target=_blank>Python
notebook</a>
used to prepare the lecture. You can view it <a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Notebooks/pandas_id3.html rel=external target=_blank>here</a></li></ul><h2 id=reading-guide>Reading Guide</h2><h3 id=information-theory>Information theory.</h3><p>You do not need to know much about information theory to apply the
ID3 algorithm, but the first five chapters of
<strong>An introduction to information theory: symbols, signals & noise</strong>
by Pierce, John R will give some
background to people who are interested. The book is available
online at the <a href=http://www.ub.uu.se rel=external target=_blank>University library</a>.</p><h3 id=decision-trees>Decision Trees</h3><ul><li><p><a href=http://themlbook.com/ rel=external target=_blank>The Hundred-Page Machine Learning Book</a>
<a href="https://www.dropbox.com/s/qiq2c85cle9ydb6/Chapter3.pdf?dl=0" rel=external target=_blank>Section 3.3</a></p></li><li><p>See the excellent <a href=http://www.cse.chalmers.se/~richajo/dit866/lectures/l1/decision_trees.pdf rel=external target=_blank>notes</a> by Richard Johansson from Chalmers</p></li><li><p>Section 5.3 of <a href=https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview rel=external target=_blank>A Hands-On Introduction to Machine
Learning</a></p></li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>What are decision trees?</li><li>What is Shannon entropy and how does it measure information?</li><li>How do I calculate the Shannon entropy of a distribution?</li><li>How does the ID3 learning algorithm work?</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=lecture-9-principle-component-analysis-and-preprocessing>Lecture 9: Principle Component Analysis and Preprocessing</h1><h2 id=todays-topics>Today&rsquo;s Topics</h2><p>As we have seen before it is often a good idea to do some
pre-processing of your input data. Sometimes there are linear
relationships hidden in the data, and if you do not remove or discover
them then your machine learning algorithm will be trying to learn
these linear relationships. In linear algebra, principle component
analysis (PCA) is a well established method of reducing the dimension
of a data set. It uses eigenvalues and eigenvectors to uncover hidden
linear relationships in your data-set. For this course you do not need
to know how to actually compute eigenvalues and eigenvectors by hand,
but you should understand the definition and what is going on.</p><p>An interesting application of PCA are
<a href=https://en.wikipedia.org/wiki/Eigenface rel=external target=_blank>Eigenfaces</a>.</p><h2 id=slides>Slides</h2><p>I use <a href=https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture9.pdf rel=external target=_blank>these
slide</a>. In
previous years, I have done the lecture on the blackboard, and these
are some
<a href=https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture9_handwritten.pdf rel=external target=_blank>notes</a>
that I used. They contain some extra derivations.</p><h2 id=reading-guide>Reading Guide</h2><ul><li><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a>
<a href="https://www.dropbox.com/s/y9a7b0hzmuksqar/Chapter9.pdf?dl=0" rel=external target=_blank>Chapter 9 section 9.3</a></li><li>Chapter 8 of <a href=https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview rel=external target=_blank>A Hands-On Introduction to Machine
Learning</a>.</li><li><a href=https://uub.primo.exlibrisgroup.com/permalink/46LIBRIS_UUB/d23b4h/alma991018444332907596 rel=external target=_blank>A First Course in Machine
Learning</a> 7.1,7.2 has a good
derivation of PCA from the point of view of minimising the
variance in the data.</li><li><a href=https://medium.com/apprentice-journal/pca-application-in-machine-learning-4827c07a61db rel=external target=_blank>PCA in Machine
Learning</a><ul><li><a href=https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html rel=external target=_blank>Principal Component
Analysis</a></li><li><a href=https://towardsdatascience.com/pca-eigenvectors-and-eigenvalues-1f968bc6777a rel=external target=_blank>Towards Data Science on PCA</a></li></ul></li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>What is principle competent analysis (PCA)?</li><li>What is the co-variance matrix and what do the entries mean?</li><li>What do the eigenvalues of the co-variance matrix tell you about the
data?</li><li>How do you choose the number of dimensions in PCA?</li><li>What are some applications of PCA in machine learning?</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=lecture-10-ensemble-learning>Lecture 10: Ensemble Learning</h1><h2 id=slides>Slides</h2><p>Link for the
<a href=https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture10.pdf rel=external target=_blank>slides.</a></p><h2 id=todays-topics-ensemble-learning>Today&rsquo;s Topics: Ensemble Learning</h2><p>Ensemble learning is a simple idea. Instead of training one model, we
train multiple models and combine the results.
A popular ensemble learning
algorithm is random forests that combines many decision trees that are
trained on random subsets of the features. To build a classifier a
majority vote is taken. There are various techniques to improve the
system including boosting, bagging and gradient boosting. As you will
see below these are not limited to random forests, but to other
combinations of learning algorithms. There is a lot extensions and
refinements of ensemble methods including AdaBoost. We will only
cover gradient boosting and not go too much into the mathematics
behind ensemble learning (that is reserved for a more advanced
course). The aim of this lecture is to give you access to another
powerful machine learning technique.</p><p>One thing that we did not cover in <a href=/lectures/lecture8/index.html>Lecture 8</a> was using decision trees for
regression. Although the idea is not that complicated you should spend
some time understand how Regression trees can be used for
regression. Although you will not be examined on other algorithms for
constructing decisions trees (we covered ID3 in <a href=/lectures/lecture8/index.html>Lecture 8</a> ) you should be aware that
there are other algorithms.</p><p>Constructing the perfect decision tree is
a computationally hard problem (in fact it is NP-complete). For
machine learning we want fast and efficient algorithms for learning,
and most decision tree algorithms in the literature are
approximations.</p><h2 id=reading-guide>Reading Guide</h2><ul><li><p><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a> <a href="https://www.dropbox.com/s/qiq2c85cle9ydb6/Chapter3.pdf?dl=0" rel=external target=_blank>Section
3.3</a> is
a good start on decision trees for regression. Again the book does
not really go into much detail on the algorithms, and it only a
starting point. The wikipedia page on
<a href=https://en.wikipedia.org/wiki/Decision_tree_learning rel=external target=_blank>Decision tree
learning</a> is a
good starting point and has many useful references to different
learning algorithms.</p></li><li><p><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a>
<a href="https://www.dropbox.com/s/esprbgjm0wc5afz/Chapter7.pdf?dl=0" rel=external target=_blank>Chapter
7.5</a> is
a good start on ensemble methods but does not go into much
detail. As always with the book, if you do not understand the
explanation then start looking at the references.</p></li><li><p>Section 5.4 of <a href=https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview rel=external target=_blank>A Hands-On Introduction to Machine
Learning</a>
cover random forests.</p></li><li><p>Chapter 8 of <a href=https://www.bonaccorso.eu/books/ rel=external target=_blank>Machine Learning Algorithms</a>
(Bonaccorso, Giuseppe) <a href=http://tinyurl.com/qsse4vb rel=external target=_blank>online university library
link</a> contains a very good overview of
decision trees and random forests. There are also lots of
scikit-learn code fragments that you can use in your own projects.</p></li><li><p><a href=https://www.kaggle.com/prashant111/bagging-vs-boosting rel=external target=_blank>A good Kaggle notebook on Bagging and
Boosting</a></p></li><li><p>To explore more than is covered in the book on gradient boosting you
can start at the <a href=https://en.wikipedia.org/wiki/Gradient_boosting rel=external target=_blank>Wikipedia
page</a> and follow
the references.</p></li><li><p>For your project you should explore scikit-learn
<a href=https://scikit-learn.org/stable/modules/ensemble.html rel=external target=_blank>API</a> on
ensemble methods.</p></li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>How do I use decision trees for regression?</li><li>What is ensemble learning?</li><li>How do random forest work?</li><li>What is boosting and bagging?</li><li>What is gradient boosting?</li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=lecture-11-ethics-and-bias>Lecture 11: Ethics and Bias</h1><h2 id=slides>Slides</h2><p>Link for the
<a href=https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture11.pdf rel=external target=_blank>slides.</a></p><h2 id=todays-topics-ethics-bias-and-the-law>Today&rsquo;s Topics: Ethics, Bias and the Law</h2><ul><li>Introduction to ethics in AI and computer science</li><li>Definitions and introduction to bias.</li><li>Some technical discussions on how to avoid bias.</li><li>Introduction to EU laws on AI.</li></ul><h2 id=reading-guide>Reading Guide</h2><p>There are a lot of links in the slides (email me if they are
broken). You are encouraged to read them. The whole EU AI Act is long
and written for lawyers. You should read <a href=https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai rel=external target=_blank>Ethics guidelines for
trustworthy
AI</a> instead.</p><p>You should also read chapter 13 of <a href=https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview rel=external target=_blank>A Hands-On Introduction to Machine
Learning</a>.</p><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>Definitions of Bias</li><li>Understand basic ethical problems within machine learning and AI.</li><li>Understand technical solutions to bias and the limitations.</li><li>Some understanding of the AI framework for AI, and what it means to
you as a future machine learning engineer.</li></ul><p>This material might be examined (I haven&rsquo;t examined it in previous
years, this material was new in 2024). One way of examining the
material is to present a simple case study, and ask you to discuss
bias and how the AI framework would have an impact on you as a machine
learning engineer. A good place to start would be to think about the
Amazon recruiting system.</p><footer class=footline></footer></article></section></div></main></div><script src=/js/clipboard.min.js?1740928677 defer></script><script src=/js/perfect-scrollbar.min.js?1740928677 defer></script><script>function useMathJax(e){window.MathJax=Object.assign(window.MathJax||{},{tex:{inlineMath:[["\\(","\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]]},options:{enableMenu:!1}},e)}useMathJax(JSON.parse("{}"))</script><script id=MathJax-script async src=/js/mathjax/tex-mml-chtml.js?1740928677></script><script src=/js/theme.js?1740928677 defer></script></body></html>