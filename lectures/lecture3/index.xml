<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Lecture 3: Probability and Naive Bayes Classification :: Introduction to ML 1D034 homepage.</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture3/index.html</link><description>Today’s Topic Using Bayes’ theorem for machine learning. You should do some revision on the use of Bayes’ theorem in general. In this lecture you will look at how to use Bayes’ theorem to build a spam detector. One important idea to take away from this lecture is that there are a variety of ways implementing spam detection: in particular there are different feature models that you can use that give you different ways of calculating the relevant probabilities. It is important that you understand the difference between the different ways of implementing spam detection.</description><generator>Hugo</generator><language>en</language><managingEditor>justin.pearson@it.uu.se (Justin Pearson)</managingEditor><webMaster>justin.pearson@it.uu.se (Justin Pearson)</webMaster><atom:link href="https://intro-ml-1dl034-uu-se.github.io/lectures/lecture3/index.xml" rel="self" type="application/rss+xml"/><item><title>Naive Bayes for Spam Classification</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture3/naive_bayes_spam/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture3/naive_bayes_spam/index.html</guid><description>There are a lot of tutorials and youtube videos out there on using Naive Bayes for document classification. None of these tutorials are wrong, but they often hide some subtle points that if you think too hard about you will get confused. In this posts I want to explain what is really going on in Naive Bayes for spam classification.
This post assumes that you are already familiar with Bayes’ theorem.</description></item></channel></rss>