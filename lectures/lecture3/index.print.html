<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.123.3">
    <meta name="generator" content="Relearn 5.24.2+tip">
    <meta name="description" content="">
    <meta name="author" content="Justin Pearson">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Lecture 3: Probability and Naive Bayes Classification :: Introduction to ML 1D034 homepage.">
    <meta name="twitter:description" content="">
    <meta property="og:title" content="Lecture 3: Probability and Naive Bayes Classification :: Introduction to ML 1D034 homepage.">
    <meta property="og:description" content="">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://intro-ml-1dl034-uu-se.github.io/lectures/lecture3/index.html">
    <meta property="og:site_name" content="Introduction to ML 1D034 homepage.">
    <title>Lecture 3: Probability and Naive Bayes Classification :: Introduction to ML 1D034 homepage.</title>
    <link href="https://intro-ml-1dl034-uu-se.github.io/lectures/lecture3/index.html" rel="canonical" type="text/html" title="Lecture 3: Probability and Naive Bayes Classification :: Introduction to ML 1D034 homepage.">
    <link href="/lectures/lecture3/index.xml" rel="alternate" type="application/rss+xml" title="Lecture 3: Probability and Naive Bayes Classification :: Introduction to ML 1D034 homepage.">
    <!-- https://github.com/filamentgroup/loadCSS/blob/master/README.md#how-to-use -->
    <link href="/css/fontawesome-all.min.css?1709150364" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/fontawesome-all.min.css?1709150364" rel="stylesheet"></noscript>
    <link href="/css/nucleus.css?1709150364" rel="stylesheet">
    <link href="/css/auto-complete.css?1709150364" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/auto-complete.css?1709150364" rel="stylesheet"></noscript>
    <link href="/css/perfect-scrollbar.min.css?1709150364" rel="stylesheet">
    <link href="/css/fonts.css?1709150364" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/fonts.css?1709150364" rel="stylesheet"></noscript>
    <link href="/css/theme.css?1709150364" rel="stylesheet">
    <link href="/css/theme-blue.css?1709150364" rel="stylesheet" id="R-variant-style">
    <link href="/css/chroma-learn.css?1709150364" rel="stylesheet" id="R-variant-chroma-style">
    <link href="/css/variant.css?1709150364" rel="stylesheet">
    <link href="/css/print.css?1709150364" rel="stylesheet" media="print">
    <link href="/css/format-print.css?1709150364" rel="stylesheet">
    <link href="/css/ie.css?1709150364" rel="stylesheet">
    <script src="/js/url.js?1709150364"></script>
    <script src="/js/variant.js?1709150364"></script>
    <script>
      // hack to let hugo tell us how to get to the root when using relativeURLs, it needs to be called *url= for it to do its magic:
      // https://github.com/gohugoio/hugo/blob/145b3fcce35fbac25c7033c91c1b7ae6d1179da8/transform/urlreplacers/absurlreplacer.go#L72
      window.index_js_url="/index.search.js";
      var root_url="/";
      var baseUri=root_url.replace(/\/$/, '');
      window.relearn = window.relearn || {};
      window.relearn.baseUriFull='https:\/\/intro-ml-1dl034-uu-se.github.io/';
      // variant stuff
      window.relearn.themeVariantModifier='';
      window.variants && variants.init( [ 'blue' ] );
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
    </script>
  </head>
  <body class="mobile-support print" data-url="/lectures/lecture3/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="/index.html"><span itemprop="name">Introduction to Machine Learning 1DL034</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="/lectures/index.html"><span itemprop="name">Lectures</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><span itemprop="name">Lecture 3: Probability and Naive Bayes Classification</span><meta itemprop="position" content="3"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable default" tabindex="-1">
        <div class="flex-block-wrapper">
          <article class="default">
            <header class="headline">
            </header>
<h1 id="lecture-3-probability-and-naive-bayes-classification">Lecture 3: Probability and Naive Bayes Classification</h1>

<h2 id="todays-topic">Today&rsquo;s Topic</h2>
<p>Using Bayes&rsquo; theorem for machine learning. You should do some revision
on the use of Bayes&rsquo; theorem in general. In this lecture you will look
at how to use Bayes&rsquo; theorem to build a spam detector. One important
idea to take away from this lecture is that there are a variety of
ways implementing spam detection: in particular there are different
feature models that you can use that give you different ways of
calculating the relevant probabilities.   It is important that you
understand the difference between the different ways of implementing
spam detection.</p>
<h2 id="reading-guide">Reading Guide</h2>
<ul>
<li>
<p><a href="https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture3.pdf" target="_blank">Lecture
Slides</a></p>
</li>
<li>
<p><a href="https://www.dropbox.com/s/0cprdghmnzpck8h/Chapter2.pdf?dl=0" target="_blank">Chapter
2</a>
of <a href="http://themlbook.com/" target="_blank">The Hundred-Page Machine Learning Book</a>
contains some background on probability and Bayes&rsquo; theorem.</p>
</li>
<li>
<p>My <a href="/lectures/lecture3/naive_bayes_spam/index.html">notes</a> on Naive Bayes for
spam detection talk about the different ways of calculating the
probabilities involved.</p>
<ul>
<li><a href="https://courses.cs.washington.edu/courses/cse312/18sp/lectures/naive-bayes/naivebayesnotes.pdf" target="_blank">Jonathan Lee&rsquo;s notes on Naive Bayes for Spam
Filtering</a>
notes are a bit more mathematical.</li>
</ul>
</li>
</ul>
<h2 id="what-should-i-know-by-the-end-of-this-lecture">What should I know by the end of this lecture?</h2>
<ul>
<li>How do I use Bayes&rsquo; theorem?</li>
<li>How do I use Bayes&rsquo; theorem to build a simple spam detector that
only uses one word?</li>
<li>What is independence assumption in the naive Bayes&rsquo; algorithm?</li>
<li>What are the various models  estimating the probabilities for spam
detection?</li>
<li>What is Laplacian smoothing and how does it work in the context of
spam detection?</li>
<li>When do you need to use logarithms to calculate the relevant
probabilities, and how do you use them?</li>
</ul>

            <footer class="footline">
            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of Lecture 3: Probability and Naive Bayes Classification</h1>
          <article class="default">
            <header class="headline">
            </header>
<h1 id="naive-bayes-for-spam-classification">Naive Bayes for Spam Classification</h1>

<p>There are a lot of tutorials and youtube videos out there on using
Naive Bayes for document classification. None of these tutorials are
wrong, but they often hide some subtle points that if you think too
hard about you will get confused. In this posts I want to explain what
is really going on in Naive Bayes for spam classification.</p>
<p>This post assumes that you are already familiar with <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" target="_blank">Bayes&rsquo; theorem</a>.</p>
<p>Rather foolishly I did all the calculations in the post by hand. If
you find any errors the please report them to me.</p>
<h2 id="our-data-set">Our data set</h2>
<p>To make things more concrete we will work on a very small data set
where we can do the calculations directly. We are classifying
micro-tweets of exactly 3 words. Our training set is as follows. 
<span class="math align-center">$S$</span>
indicates that a message is spam and 
<span class="math align-center">$\overline{S}$</span> indicates that a
message is not spam.</p>
<table>
<thead>
<tr>
<th>Number</th>
<th>Tweet</th>
<th>Spam (
<span class="math align-center">$S$</span> or 
<span class="math align-center">$\overline{S}$</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>money aardvark boondoggle</td>
<td>
<span class="math align-center">$S$</span></td>
</tr>
<tr>
<td>2</td>
<td>money money money</td>
<td>
<span class="math align-center">$S$</span></td>
</tr>
<tr>
<td>3</td>
<td>money money world</td>
<td>
<span class="math align-center">$S$</span></td>
</tr>
<tr>
<td>4</td>
<td>money world world</td>
<td>
<span class="math align-center">$S$</span></td>
</tr>
<tr>
<td>5</td>
<td>viagra money back</td>
<td>
<span class="math align-center">$S$</span></td>
</tr>
<tr>
<td>6</td>
<td>viagra heart honey</td>
<td>
<span class="math align-center">$S$</span></td>
</tr>
<tr>
<td>7</td>
<td>aardvark boondoggle world</td>
<td>
<span class="math align-center">$\overline{S}$</span> (not spam)</td>
</tr>
<tr>
<td>8</td>
<td>honey honey honey</td>
<td>
<span class="math align-center">$\overline{S}$</span> (not spam)</td>
</tr>
<tr>
<td>9</td>
<td>viagra heart money</td>
<td>
<span class="math align-center">$\overline{S}$</span> (not spam)</td>
</tr>
<tr>
<td>10</td>
<td>money honey now</td>
<td>
<span class="math align-center">$\overline{S}$</span> (not spam)</td>
</tr>
</tbody>
</table>
<h2 id="background-classifying-with-only-one-word">Background: Classifying with only one word.</h2>
<p>As a warm up let&rsquo;s just build a classifier that uses one particular
word 
<span class="math align-center">$w = \mathrm{money}$</span>.</p>
<p>Bayes&rsquo; Theorem should be familiar to you by now:

<span class="math align-center">$$
P(S|w) = \frac{P(w|S)P(S)}{P(w)}
$$</span></p>
<p>
<span class="math align-center">$P(S|w)$</span> is the even that an email is Spam given that the word 
<span class="math align-center">$w$</span>
occurs in it. Using Bayes theorem we can calculate the probability
that a message containing the word 
<span class="math align-center">$w$</span> is spam.  We can estimate the
values 
<span class="math align-center">$P(w|S)$</span>, 
<span class="math align-center">$P(S)$</span> and 
<span class="math align-center">$P(w)$</span> from out data set.</p>

<span class="math align-center">$$
P(S) = \frac{\mathrm{number\ spam}}{\mathrm{total\ messages}} = \frac{6}{10}
$$</span>
<p>To estimate 
<span class="math align-center">$P(w|S)$</span> we have to count the number of times that a
particular word occurs in a spam message. So

<span class="math align-center">$$
P(\mathrm{money}|S) = \frac{5}{6}
$$</span>
When you are only considering a single word then estimating 
<span class="math align-center">$P(\mathrm{money})$</span>
is easy. It is the ratio of the number of tweets that contain the
word &lsquo;money&rsquo; and the total number of tweets. The word money appears in

<span class="math align-center">$7$</span> tweets. So

<span class="math align-center">$$
P(\mathrm{money}) = \frac{7}{10}
$$</span></p>
<p>So if we get a message that contains the word &lsquo;money&rsquo; we can calculate
the probability that it is a spam message.  
<span class="math align-center">$$ P(S|\mathrm{money}) =
\frac{P(w|S)P(S)}{P(w)} = \frac{\frac{5}{6}\frac{6}{10}}{\frac{7}{10}}
= \frac{5}{10}\frac{10}{7} = \frac{5}{7} \approx 0.71 $$</span> There is an
important identity that will become useful later on 
<span class="math align-center">$$
P(\mathrm{money}) = P(\mathrm{money}|S)P(S) +
P(\mathrm{money}|\overline{S})P(\overline{S}) $$</span> So 
<span class="math align-center">$$
P(\mathrm{money})= \frac{5}{6}\frac{6}{10} + \frac{2}{4}\frac{4}{10} =
\frac{7}{10} $$</span></p>
<h2 id="first-pitfall-estimating-the-probabilities">First Pitfall: Estimating the probabilities</h2>
<p>So how to we estimate the probabilities 
<span class="math align-center">$P(w|S)$</span> , 
<span class="math align-center">$P(S)$</span> and 
<span class="math align-center">$P(w)$</span>?
What do they really mean?  The probabilities 
<span class="math align-center">$P(S)$</span> and 
<span class="math align-center">$P(\overline{S}$</span>) are
unambiguous. They are just the probability that a tweet is spam or
not. But 
<span class="math align-center">$P(w|S)$</span>, 
<span class="math align-center">$P(w|\overline{S})$</span>, and 
<span class="math align-center">$P(w)$</span> can mean different things
depending exactly which model we use to calculate the probabilities.</p>
<p>There are two models:</p>
<ul>
<li>
<p>(A): To calculate 
<span class="math align-center">$P(\mathrm{money}|S)$</span>. There are 6 messages that
are spam and in those 6 messages 5 of them (1,2,3,4,5) contain the word money so

<span class="math align-center">$P(\mathrm{money}|S) = 5/6$</span>, and of the 10 messages 7 of them
(1,2,3,4,5,9,10)contain the word &lsquo;money&rsquo; so 
<span class="math align-center">$P(\mathrm{money}) =
7/10$</span>. This is exactly what we did above.</p>
</li>
<li>
<p>(B): To calculate 
<span class="math align-center">$P(\mathrm{money})$</span> there are 
<span class="math align-center">$10\times 3 = 30$</span>
words in our training set and the word money appears 10 times so

<span class="math align-center">$$P(\mathrm{money}) = 10/30.$$</span> To calculate 
<span class="math align-center">$P(\mathrm{money}|S)$</span>
there are 6 spam messages each of 3 words long. In the words of the
spam messages the word &lsquo;money&rsquo; appears 8 times. So 
<span class="math align-center">$$
   P(\mathrm{money} | S) = \frac{8}{3 \times 6} = \frac{8}{18} =
   \frac{4}{9} $$</span>
The probability a message being spam is still

<span class="math align-center">$6/10$</span>.    So
if I get the message &lsquo;money for nothing&rsquo; then the probability that
it is spam is calculated as before 
<span class="math align-center">$$ P(S|\mathrm{money}) =
   \frac{P(\mathrm{money}|S)P(S)}{P(\mathrm{money})} =
   \frac{{\frac{8}{3 \times 6}\times \frac{6}{10}}}{ \frac{10}{30}} =
   \frac{8}{10} $$</span>
It seems that if spammers are prone to repeat words in their
message then this increases the probability that a message
containing that word is spam.</p>
</li>
</ul>
<p>So how do you calculate the probability that the message &lsquo;money money
mammon&rsquo; is spam? In model (A) it does not matter how many times &lsquo;money&rsquo;
appears in a message: you only count the number of messages &lsquo;money&rsquo;
appears in. While in model (B) there is some weighting of the number
of times that a word appears. But to calculate

<span class="math align-center">$P(S|\mathrm{money}^2)$</span> (where  is short hand for &lsquo;money appearing
twice) we have calculate 
<span class="math align-center">$P(\mathrm{money}^2|S)$</span>. How you do this
depends a bit on your model and the assumptions underlying the
model. We&rsquo;ll get to that later.</p>
<h2 id="take-home-message">Take home message</h2>
<p>So the first take home message is be careful how you count the words
and how you calculate the probabilities.  If you confuse model (A) and
model (B) while writing your code you will get strange answers (as I
did at one point).</p>
<h2 id="naive-bayes-first-version">Naive Bayes: first version</h2>
<p>We are going to use model (A). That is we going to ignore how many
times a word appears in a message. We are only interested if the word
appears on the message or not.
One word is not going to much of a spam classifier. Even in our little
data set above, the word &lsquo;money&rsquo; can appear in spam and non spam
messages. We will get a better classifier if we take into account more
words. Our data set is quite small and for each word we can count the
number of times it appears in a spam tweet and the number of times it
appears in a non-spam tweet.</p>
<table>
<thead>
<tr>
<th>Word</th>
<th>occurrences  in spam</th>
<th>occurrences in  non spam</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span class="math align-center">$w_1 = \mathrm{money}$</span></td>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td>
<span class="math align-center">$w_2 = \mathrm{world}$</span></td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>
<span class="math align-center">$w_3 = \mathrm{viagra}$</span></td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>
<span class="math align-center">$w_4 = \mathrm{aardvark}$</span></td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>
<span class="math align-center">$w_5 = \mathrm{heart}$</span></td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>
<span class="math align-center">$w_6 = \mathrm{boondoggle}$</span></td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>
<span class="math align-center">$w_7 = \mathrm{honey}$</span></td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>
<span class="math align-center">$w_8 = \mathrm{back}$</span></td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>
<span class="math align-center">$w_9 = \mathrm{now}$</span></td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>You can turn these counts into probabilities, and thus you can
calculate quantities like 
<span class="math align-center">$P(\mathrm{money}|S) =  5/6$</span>,

<span class="math align-center">$P(\mathrm{money}|\overline{S}) = 2/4$</span>.</p>
<p>Suppose I receive a message &lsquo;viagra money boondoggle&rsquo; what is the
probability that it is spam message? When we use Bayes&rsquo; theorem we
have to calculate 
<span class="math align-center">$$P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle}|S)$$</span> where 
<span class="math align-center">$$\mathrm{viagra} \land \mathrm{money}
\land \mathrm{boondoggle}$$</span> is the event that the words &lsquo;viagra&rsquo;,
&lsquo;money&rsquo; and &lsquo;boondoggle&rsquo; appears in a message.</p>
<h2 id="the-naive-in-naive-bayes">The Naive in Naive Bayes</h2>
<p>We need to make an independence assumption. In a spam or non spam
message the probability of words are independent. That is

<span class="math align-center">$$
P(w_1 \land w_2 \land \cdots \land w_n | S) = P(w_1|S)P(w_1|S) \cdots
P(w_n|S)
$$</span>
and

<span class="math align-center">$$
P(w_1 \land w_2 \land \cdots \land w_n | \overline{S}) = P(w_1|\overline{S})P(w_1|\overline{S}) \cdots
P(w_n|\overline{S})
$$</span></p>
<p>Note this is a weaker assumption than simply saying

<span class="math align-center">$$
P(w_1 \cdots w_n) = \prod_{1\leq i \leq n} P(w_i)
$$</span></p>
<h2 id="take-home-message-1">Take home message</h2>
<p>Note that because we have made the assumptions that

<span class="math align-center">$$P(w_1 \land w_2
\land \cdots \land w_n | S) =  
\prod_{i=1}^n P(w_i|S)$$</span> and

<span class="math align-center">$$P(w_1 \land w_2 \land \cdots \land w_n | S) = 
\prod_{i=1}^n P(w_i|S)$$</span>
it does not make sense to directly estimate the probabilities of

<span class="math align-center">$P(w_i)$</span> directly from the data set. Later on we will see that you do
actually need the probabilities 
<span class="math align-center">$P(w_1 \cdots w_n)$</span> to decide if a
message is spam or not. If you want to calculate the probability

<span class="math align-center">$P(w_1 \cdots w_n)$</span> then you must use the identity 
<span class="math align-center">$P(w_1 \cdots
w_n)$</span> equals

<span class="math align-center">$$
P(w_1 \cdots w_n|S)P(S) + P(w_1 \cdots w_n|\overline{S})P(\overline{S})
$$</span></p>
<p>The independence assumption is why Naive Bayes is referred to as
naive. Although this model could be improved. It seems that the
probability of one word appearing in a message should not be
independent of another word. If a spammer write &lsquo;money&rsquo; then he is
likely to also include &lsquo;viagra&rsquo; in the message. Even so, assuming
independence works very well in practice.</p>
<h2 id="calculating-the-spam-probability">Calculating the spam probability.</h2>
<p>We can now apply Bayes&rsquo; theorem 
<span class="math align-center">$P(S | \mathrm{viagra} \land
\mathrm{money} \land \mathrm{boondoggle})$</span> equals 
<span class="math align-center">$$ \frac{
P(\mathrm{viagra} \land \mathrm{money} \land \mathrm{boondoggle}|S)
P(S)}{P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle})} $$</span></p>
<p>From the independence assumption we have that 
<span class="math align-center">$P(S | \mathrm{viagra}
\land \mathrm{money} \land \mathrm{boondoggle})$</span> equals 
<span class="math align-center">$$ \frac{
P(\mathrm{viagra}|S)P(\mathrm{money}|S)P(\mathrm{boondoggle}|S)P(S)}
{P(\mathrm{viagra} \land \mathrm{money} \land \mathrm{boondoggle})} $$</span></p>
<p>To  calculate 
<span class="math align-center">$P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle})$</span> we use the identity above.
Taking product 
<span class="math align-center">$$P(\mathrm{viagra})P(\mathrm{money})P(\mathrm{boondoggle})$$</span>
is the wrong answer.</p>
<p>So instead we get that 
<span class="math align-center">$P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle})$</span> equals

<span class="math align-center">$P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle}|S)P(S)$</span> plus 
<span class="math align-center">$P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle}|\overline{S})P(\overline{S})$</span>.</p>
<p>The by the independence assumption

<span class="math align-center">$P(\mathrm{viagra} \land \mathrm{money} \land \mathrm{boondoggle}|S)$</span> equals</p>

<span class="math align-center">$$
P(\mathrm{viagra}|S)P(\mathrm{money}|S)P(\mathrm{boondoggle}|S) = 
\frac{2}{6}\frac{5}{6}\frac{1}{6} = \frac{5}{108} 
$$</span>
<p>Putting the numbers in we get 
<span class="math align-center">$P(\mathrm{viagra} \land \mathrm{money} \land
\mathrm{boondoggle})$</span> equals

<span class="math align-center">$$ \left(\frac{2}{6}\cdot \frac{5}{6}\cdot
\frac{1}{6}\right)\frac{6}{10} + \left(\frac{1}{4}\cdot
\frac{2}{4}\cdot \frac{1}{4}\right)\frac{4}{10} \approx 0.08
$$</span></p>
<p>So 
<span class="math align-center">$P(S|\mathrm{viagra} \land
\mathrm{money} \land \mathrm{boondoggle})$</span> equals</p>

<span class="math align-center">$$
\frac{\frac{5}{108}\frac{6}{10}}{0.08} \approx 0.35
$$</span>
<h2 id="not-calculating-the-whole-probability">Not calculating the whole probability.</h2>
<p>When implementing the spam filter we do not actually need to calculate
the denominators. We just compare the expressions

<span class="math align-center">$P(\mathrm{viagra}|S)P(\mathrm{money}|S)P(\mathrm{boondoggle}|S)P(S)$</span>
and

<span class="math align-center">$P(\mathrm{viagra}|\overline{S})P(\mathrm{money}|\overline{S})P(\mathrm{boondoggle}|\overline{S})P(\overline{S})$</span>
and see which one is bigger. This is also important because some of
the numbers start getting smaller and smaller and you end up with
floating point underflow errors. If the numbers get too small then you
have to calculate with the logarithm of the probability and do
additions rather than subtraction.</p>
<h2 id="laplacian-smoothing">Laplacian Smoothing</h2>
<p>What if we have the message &rsquo;now money viagra&rsquo;, if we look at our data
set the word &rsquo;now&rsquo; has not appeared in a spam message. There could be
two reasons for this, one is that a spam message will never contain
the word &rsquo;now&rsquo; (unlikely), or that we just do not have a spam message
with &rsquo;now&rsquo; appearing in our training set. If we use model (A) and
calculate the probability that our message is spam we get

<span class="math align-center">$P(S|\mathrm{now}\land\mathrm{money}\land{viagra})$</span> equals

<span class="math align-center">$$
\frac{P(\mathrm{now}|S)P(\mathrm{money}|S)P(\mathrm{viagra}|S)P(S)}{P(\mathrm{now}\land\mathrm{money}\land\mathrm{viagra})}
$$</span></p>
<p>which equals

<span class="math align-center">$$\frac{0 \cdot \frac{5}{6} \cdot
\frac{2}{6}\frac{6}{10}}{P(\mathrm{now}\land\mathrm{money}\land\mathrm{viagra})}
$$</span>
So even though the words &lsquo;money&rsquo; and &lsquo;viagra&rsquo; are pretty good
indicators of a message being spam we get probability 0.</p>
<p>To get around this we add one to all our counts to avoid probability

<span class="math align-center">$0$</span> estimates and adjust the total count so as to avoid any
probabilities greater than 
<span class="math align-center">$1$</span>. So in model (A) if we are considering

<span class="math align-center">$9$</span> words as a above then we estimate 
<span class="math align-center">$P(\mathrm{now}|S)$</span> to be

<span class="math align-center">$$
\frac{0 + 1}{6 + 1}
$$</span>
instead of

<span class="math align-center">$$
\frac{0}{6}
$$</span>
If you had a word that appeared in all 6 of the spam tweets then you
would get an estimate 
<span class="math align-center">$\frac{6+1}{6+1}$</span> which would be 
<span class="math align-center">$1$</span>.</p>
<p>I leave it an exercise to work out the correct thing to do in model
(B).</p>
<h2 id="feature-modelling">Feature Modelling</h2>
<p>All most all machine learning algorithms require numbers and vectors as
inputs. Our Naive Bayes classifier does not really work with words,
but feature vectors. There are different possible models, but we use
something similar to model (A). First we take our data set and find
the first 
<span class="math align-center">$n$</span> most popular words. The most popular word a data set
consisting of English messages is typically the word &rsquo;the&rsquo;. You can
improve things by filtering out the most popular words that don&rsquo;t
contribute much a message being (referred to as stop words). We will
not worry about that here. But a word like &rsquo;the&rsquo; is equally likely to
appear in a spam tweet or a non spam tweet, so it is better to ignore
it.</p>
<p>Then we turn each message into a vector of length 
<span class="math align-center">$n$</span> where each entry
is 
<span class="math align-center">$1$</span> or 
<span class="math align-center">$0$</span>, and the 
<span class="math align-center">$i$</span>th entry is 
<span class="math align-center">$1$</span> if the message contains the

<span class="math align-center">$i$</span>th most popular word. So in a typical English message data set
&rsquo;the&rsquo; is the most popular and &rsquo;to&rsquo; is the second most popular word. So
if our message contained the words &rsquo;to the&rsquo; then the first two entries
of its feature vector would have the value 
<span class="math align-center">$1$</span>.  
<span class="math align-center">$$ f = (f_1, \ldots ,
f_i, \ldots, f_n) $$</span> Where 
<span class="math align-center">$f_i$</span> is 
<span class="math align-center">$1$</span> if the 
<span class="math align-center">$i$</span>th most popular word

<span class="math align-center">$w_i$</span> occurs in the message and 
<span class="math align-center">$0$</span> otherwise.</p>
<p>It is easy to write a function takes a message and turns it into our
feature vector. Given our training set we can estimate two
probabilities for our two classes 
<span class="math align-center">$S$</span> and 
<span class="math align-center">$\overline{S}$</span>,  
<span class="math align-center">$P(w_i|S)$</span>, 
<span class="math align-center">$P(\overline{w}_i|S)$</span>, 
<span class="math align-center">$P(w_i| \overline{S})$</span> and 
<span class="math align-center">$P(\overline{w}_i |
\overline{S})$</span>, where 
<span class="math align-center">$w_i$</span> is the even that word 
<span class="math align-center">$i$</span> occurs in the
message and 
<span class="math align-center">$\overline{w}_i$</span> is the event that word 
<span class="math align-center">$i$</span> does not occur
in the message.</p>
<p>In our example above we only have 
<span class="math align-center">$9$</span> words in our data set and they
appear in order of popularity as &lsquo;money&rsquo;, &lsquo;world&rsquo;, &lsquo;viagra&rsquo;,
&lsquo;aardvark&rsquo;, &lsquo;heart&rsquo;, &lsquo;boondoogle&rsquo; , &lsquo;honey&rsquo;, &lsquo;back&rsquo; , &rsquo;now&rsquo;.  You have
to break ties (words that are equally popular) and you have to do it
consistently.  So give the message &lsquo;aardvark money now&rsquo; its feature
vector would be 
<span class="math align-center">$$ f = (1,0,0,1,0,0,0,0,1) $$</span></p>
<p>This vector 
<span class="math align-center">$f$</span> corresponds to the event

<span class="math align-center">$$
w_1\overline{w}_2\overline{w}_3w_4\overline{w}_5\overline{w}_6\overline{w}_7w_8
$$</span></p>
<p>So to use Bayes&rsquo; theorem to work on the probability that the tweet
is is spam we have to
calculate the quantity

<span class="math align-center">$$
\frac{P(w_1\overline{w}_2\overline{w}_3w_4\overline{w}_5\overline{w}_6\overline{w}_7\overline{w}_8|S)P(S)}{P(w_1\overline{w}_2\overline{w}_3w_4\overline{w}_5\overline{w}_6\overline{w}_7\overline{w}_8
w_9)}
$$</span></p>
<p>Calculating

<span class="math align-center">$P(w_1\overline{w}_2\overline{w}_3w_4\overline{w}_5\overline{w}_6\overline{w}_7\overline{w}_8|S)$</span>
is easily done by the independence assumption. It is the product of
the terms

<span class="math align-center">$P(w_1|S)$</span>,
<span class="math align-center">$P(\overline{w}_2|S)$</span>,
<span class="math align-center">$P(\overline{w}_3|S)$</span>,
<span class="math align-center">$P(w_4|S)$</span>,
<span class="math align-center">$P(\overline{w}_5|S)$</span>,
<span class="math align-center">$P(\overline{w}_6|S)$</span>,
<span class="math align-center">$P(\overline{w}_7|S)$</span>
and 
<span class="math align-center">$P(\overline{w}_8|S)$</span>. All these values are easily estimated from
our data set. For example 
<span class="math align-center">$P(\overline{w}_3|S)$</span> is the probability
that the word &lsquo;money&rsquo; does not appear in a spam tweet. We had 6 spam
tweets and 5 of them contained the word money and so we get that

<span class="math align-center">$P(\overline{w}_3|S)$</span> equals 
<span class="math align-center">$1/6$</span>.</p>
<h2 id="model-a-and-feature-vectors">Model (A) and Feature Vectors</h2>
<p>If you go back to model (A) and you try to estimate if a message is
spam or not, then using the same message we would only need to
calculate

<span class="math align-center">$$
\frac{P(w_1|S)(w_4|S)P(w_9|S)P(S)}{P(w_1\land w_w \land w_3)}
$$</span>
since  
<span class="math align-center">$w_1$</span> is &lsquo;money&rsquo;, 
<span class="math align-center">$w_4$</span> is &lsquo;aardvark&rsquo; and 
<span class="math align-center">$w_9$</span> is &rsquo;now&rsquo;.   We
are throwing away information about the words that do not occur in the
tweet. Does this matter? More importantly is this calculation
incorrect.</p>
<p>To simply things imagine that we only had two words in our feature
vector 
<span class="math align-center">$W_1$</span> and 
<span class="math align-center">$W_2$</span>. Then given a message there are 4 possible
atomic events:</p>

<span class="math align-center">$$ W_1 W_2, W_1 \overline{W}_2, \overline{W}_1 W_2, \overline{W}_1
\overline{W_2}$$</span>
<p>What do we mean when we write 
<span class="math align-center">$P(W_1|S)$</span>? Looking at our atomic
events we actually mean

<span class="math align-center">$$
P( W_1 W_2 \lor W_1\overline{W}_2|S)
$$</span>
Any event is a union of the atomic events in your probability model.
Using the independence assumption for 
<span class="math align-center">$W_1$</span> and 
<span class="math align-center">$W_2$</span> and the basic
rule of probability that 
<span class="math align-center">$P(A\lor B)$</span> equals 
<span class="math align-center">$P(A)+P(B)$</span> when then
events 
<span class="math align-center">$A$</span> and 
<span class="math align-center">$B$</span> are independent atomic events we get that

<span class="math align-center">$P(W_1|S)$</span> equals 
<span class="math align-center">$P( W_1 W_2 \lor W_1\overline{W}_2|S)$</span> which equals

<span class="math align-center">$$
P(W_1|S)P(W_2|S) + 
P(W_1|S)P(\overline{W}_2|S)$$</span> refactoring gives

<span class="math align-center">$$P(W_1|S)(P(W_2|S) + 
P(\overline{W}_2|S))
$$</span>
and since 
<span class="math align-center">$P(W_2|S) +  P(\overline{W}_2|S)$</span> equals 
<span class="math align-center">$1$</span> we get

<span class="math align-center">$P(W_1|S)$</span>.</p>
<p>So if we ignore any information about 
<span class="math align-center">$W_2$</span> then we get a factor of

<span class="math align-center">$1$</span>. If we use what information we have about 
<span class="math align-center">$W_2$</span> then we get a
better estimate for the probability. You can show that the same
applies if you have lots of words in your feature vector. So our
original model (A) is not wrong, but the feature vector model where we
take into account if a word appears or not when we are estimating the
probabilities and gives a better estimate if the word is spam or
not. Note the above argument depends on our independence assumption
(the naive in Naive Bayes).</p>
<p>If you did not look at any words then the only information that you
would have is 
<span class="math align-center">$P(S)$</span> or 
<span class="math align-center">$P(\overline{S})$</span> as you look at more words in
your feature you get a better estimate of the probability that the
message is spam or not.</p>
<h2 id="model-b-with--multiple-words">Model (B) with  multiple words</h2>
<p>How do you calculate the probability that the message &lsquo;money money
boondoggle&rsquo; is spam? We have already assumed that the probabilities of
a words occurring in a spam or non-spam tweet are independent. If we
also assume that the probability of a word appearing 
<span class="math align-center">$k$</span> times is

<span class="math align-center">$$
P(w^k|S) = P(w|S)^k
$$</span>
That is each occurrence is independent, then we can calculate the
probability that a message containing the multiple occurrences of a
word is spam or not, but only if you use model (B) to calculate the
probabilities.  You should not mix up model (A) and model (B). It does
not make sense in model (A) to ask what the probability of a word
occurring 
<span class="math align-center">$k$</span> times in a spam message is. We can only ask if a m
message contains the word or not.</p>
<p>If you want to take into account multiple words then do not use model
(A) to calculate your probabilities.</p>
<h2 id="model-b-with-multiple-words-and-feature-vectors">Model (B) with multiple words and feature vectors.</h2>
<p>The feature vector approach for model (A) considered vectors where the
entries are 
<span class="math align-center">$0$</span> or 
<span class="math align-center">$1$</span>. Given a feature vector 
<span class="math align-center">$f$</span> the entry 
<span class="math align-center">$i$</span>th
entry is 
<span class="math align-center">$1$</span> if the word appears in the message and 
<span class="math align-center">$0$</span> otherwise.</p>
<p>Instead we would have a feature vector where the 
<span class="math align-center">$ith$</span> entry tells you
how many times the word appears in the message. Thus for our message &lsquo;money money
boondoggle&rsquo; its feature vector would be (using the same ordering as
above):

<span class="math align-center">$$
(2,0,0,0,0,1,0,0,0)
$$</span></p>
<p>Again it is not hard to use the information that a word appears 
<span class="math align-center">$0$</span>
times.</p>
<h2 id="take-home-messages">Take home messages</h2>
<ul>
<li>
<p>Are you using model (A) or model (B). Don&rsquo;t get them confused,
especially when you are estimating the probabilities 
<span class="math align-center">$P(w_i|S)$</span> from
the training data.</p>
</li>
<li>
<p>Are you using the negative information, the information that a word
does not occur? It does not matter your maths is correct, but you
are not using all the information that you have.</p>
</li>
<li>
<p>To understand how this is related to other machine learning
algorithms, then you have to understand that we take a message and
construct a feature vector. Depending on if you are using model (A)
or (B) your feature vector either has 
<span class="math align-center">$0$</span> or 
<span class="math align-center">$1$</span> entries or positive
integer that tells you how many times a word occurs in a
message. Feature vectors is an example modelling that you often have
to do in machine learning. Your data set and package  does not
always come as ready packaged as a set of vectors.</p>
</li>
<li>
<p>If you watch some random video on the internet, it is not always
clear which model they are using when they calculate the
probabilities.</p>
</li>
</ul>
<p>The documentation to <a href="https://scikit-learn.org/stable/" target="_blank">scikit-learn</a>
has a nice entry on <a href="https://scikit-learn.org/stable/modules/naive_bayes.html" target="_blank">naive
Bayes</a>, which
discusses the various options on modelling as well as links to various
interesting articles on the different modelling approaches to naive
Bayes.</p>

            <footer class="footline">
            </footer>
          </article>

          </section>
        </div>
      </main>
    </div>
    <script src="/js/clipboard.min.js?1709150364" defer></script>
    <script src="/js/perfect-scrollbar.min.js?1709150364" defer></script>
    <script>
      function useMathJax( config ){
        if( !Object.assign ){
          
          return;
        }
        window.MathJax = Object.assign( window.MathJax || {}, {
          loader: {
            load: ['[tex]/mhchem']
          },
          startup: {
            elements: [
              '.math'
            ]
          },
          tex: {
            inlineMath: [
              ['$', '$'], 
              ['\\(', '\\)']
            ]
          },
          options: {
            enableMenu: false 
          }
        }, config );
      }
      useMathJax( JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="/js/mathjax/tex-mml-chtml.js?1709150364"></script>
    <script src="/js/theme.js?1709150364" defer></script>
  </body>
</html>
