<!doctype html><html lang=en dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=print><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.140.0"><meta name=generator content="Relearn 7.3.1"><meta name=description content="Slides Link for the slides.
Today’s Topics: Ensemble Learning Ensemble learning is a simple idea. Instead of training one model, we train multiple models and combine the results. A popular ensemble learning algorithm is random forests that combines many decision trees that are trained on random subsets of the features. To build a classifier a majority vote is taken. There are various techniques to improve the system including boosting, bagging and gradient boosting. As you will see below these are not limited to random forests, but to other combinations of learning algorithms. There is a lot extensions and refinements of ensemble methods including AdaBoost. We will only cover gradient boosting and not go too much into the mathematics behind ensemble learning (that is reserved for a more advanced course). The aim of this lecture is to give you access to another powerful machine learning technique."><meta name=author content="Justin Pearson"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lecture 10: Ensemble Learning :: 1DL034"><meta name=twitter:description content="Slides Link for the slides.
Today’s Topics: Ensemble Learning Ensemble learning is a simple idea. Instead of training one model, we train multiple models and combine the results. A popular ensemble learning algorithm is random forests that combines many decision trees that are trained on random subsets of the features. To build a classifier a majority vote is taken. There are various techniques to improve the system including boosting, bagging and gradient boosting. As you will see below these are not limited to random forests, but to other combinations of learning algorithms. There is a lot extensions and refinements of ensemble methods including AdaBoost. We will only cover gradient boosting and not go too much into the mathematics behind ensemble learning (that is reserved for a more advanced course). The aim of this lecture is to give you access to another powerful machine learning technique."><meta property="og:url" content="https://intro-ml-1dl034-uu-se.github.io/lectures/lecture10/index.html"><meta property="og:site_name" content="1DL034"><meta property="og:title" content="Lecture 10: Ensemble Learning :: 1DL034"><meta property="og:description" content="Slides Link for the slides.
Today’s Topics: Ensemble Learning Ensemble learning is a simple idea. Instead of training one model, we train multiple models and combine the results. A popular ensemble learning algorithm is random forests that combines many decision trees that are trained on random subsets of the features. To build a classifier a majority vote is taken. There are various techniques to improve the system including boosting, bagging and gradient boosting. As you will see below these are not limited to random forests, but to other combinations of learning algorithms. There is a lot extensions and refinements of ensemble methods including AdaBoost. We will only cover gradient boosting and not go too much into the mathematics behind ensemble learning (that is reserved for a more advanced course). The aim of this lecture is to give you access to another powerful machine learning technique."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="Lectures"><meta itemprop=name content="Lecture 10: Ensemble Learning :: 1DL034"><meta itemprop=description content="Slides Link for the slides.
Today’s Topics: Ensemble Learning Ensemble learning is a simple idea. Instead of training one model, we train multiple models and combine the results. A popular ensemble learning algorithm is random forests that combines many decision trees that are trained on random subsets of the features. To build a classifier a majority vote is taken. There are various techniques to improve the system including boosting, bagging and gradient boosting. As you will see below these are not limited to random forests, but to other combinations of learning algorithms. There is a lot extensions and refinements of ensemble methods including AdaBoost. We will only cover gradient boosting and not go too much into the mathematics behind ensemble learning (that is reserved for a more advanced course). The aim of this lecture is to give you access to another powerful machine learning technique."><meta itemprop=wordCount content="459"><title>Lecture 10: Ensemble Learning :: 1DL034</title>
<link href=https://intro-ml-1dl034-uu-se.github.io/lectures/lecture10/index.html rel=canonical type=text/html title="Lecture 10: Ensemble Learning :: 1DL034"><link href=/lectures/lecture10/index.xml rel=alternate type=application/rss+xml title="Lecture 10: Ensemble Learning :: 1DL034"><link href=/css/fontawesome-all.min.css?1737535569 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fontawesome-all.min.css?1737535569 rel=stylesheet></noscript><link href=/css/auto-complete.css?1737535569 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/auto-complete.css?1737535569 rel=stylesheet></noscript><link href=/css/perfect-scrollbar.min.css?1737535569 rel=stylesheet><link href=/css/theme.min.css?1737535569 rel=stylesheet><link href=/css/format-print.min.css?1737535569 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.relBasePath="../..",window.relearn.relBaseUri="../..",window.relearn.absBaseUri="https://intro-ml-1dl034-uu-se.github.io",window.relearn.min=`.min`,window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.themevariants=["blue"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}}))},window.relearn.markVariant=function(){var t=window.localStorage.getItem(window.relearn.absBaseUri+"/variant"),e=document.querySelector("#R-select-variant");e&&(e.value=t)},window.relearn.initVariant=function(){var e=window.localStorage.getItem(window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant(),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`</script></head><body class="mobile-support print" data-url=/lectures/lecture10/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><button class=topbar-control onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/index.html><span itemprop=name>Introduction to Machine Learning 1DL034</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/lectures/index.html><span itemprop=name>Lectures</span></a><meta itemprop=position content="2">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>Lecture 10</span><meta itemprop=position content="3"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-print" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><a class=topbar-control href=/lectures/lecture10/index.print.html title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable lectures" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=lecture-10-ensemble-learning>Lecture 10: Ensemble Learning</h1><h2 id=slides>Slides</h2><p>Link for the
<a href=https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture10.pdf rel=external target=_blank>slides.</a></p><h2 id=todays-topics-ensemble-learning>Today&rsquo;s Topics: Ensemble Learning</h2><p>Ensemble learning is a simple idea. Instead of training one model, we
train multiple models and combine the results.
A popular ensemble learning
algorithm is random forests that combines many decision trees that are
trained on random subsets of the features. To build a classifier a
majority vote is taken. There are various techniques to improve the
system including boosting, bagging and gradient boosting. As you will
see below these are not limited to random forests, but to other
combinations of learning algorithms. There is a lot extensions and
refinements of ensemble methods including AdaBoost. We will only
cover gradient boosting and not go too much into the mathematics
behind ensemble learning (that is reserved for a more advanced
course). The aim of this lecture is to give you access to another
powerful machine learning technique.</p><p>One thing that we did not cover in <a href=/lectures/lecture8/index.html>Lecture 8</a> was using decision trees for
regression. Although the idea is not that complicated you should spend
some time understand how Regression trees can be used for
regression. Although you will not be examined on other algorithms for
constructing decisions trees (we covered ID3 in <a href=/lectures/lecture8/index.html>Lecture 8</a> ) you should be aware that
there are other algorithms.</p><p>Constructing the perfect decision tree is
a computationally hard problem (in fact it is NP-complete). For
machine learning we want fast and efficient algorithms for learning,
and most decision tree algorithms in the literature are
approximations.</p><h2 id=reading-guide>Reading Guide</h2><ul><li><p>Chapter 8 of <a href=https://www.bonaccorso.eu/books/ rel=external target=_blank>Machine Learning Algorithms</a>
(Bonaccorso, Giuseppe) <a href=http://tinyurl.com/qsse4vb rel=external target=_blank>online university library
link</a> contains a very good overview of
decision trees and random forests. There are also lots of
scikit-learn code fragments that you can use in your own projects.</p></li><li><p><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a> <a href="https://www.dropbox.com/s/qiq2c85cle9ydb6/Chapter3.pdf?dl=0" rel=external target=_blank>Section
3.3</a> is
a good start on decision trees for regression. Again the book does
not really go into much detail on the algorithms, and it only a
starting point. The wikipedia page on
<a href=https://en.wikipedia.org/wiki/Decision_tree_learning rel=external target=_blank>Decision tree
learning</a> is a
good starting point and has many useful references to different
learning algorithms.</p></li><li><p><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a>
<a href="https://www.dropbox.com/s/esprbgjm0wc5afz/Chapter7.pdf?dl=0" rel=external target=_blank>Chapter
7.5</a> is
a good start on ensemble methods but does not go into much
detail. As always with the book, if you do not understand the
explanation then start looking at the references.</p></li><li><p><a href=https://www.kaggle.com/prashant111/bagging-vs-boosting rel=external target=_blank>A good Kaggle notebook on Bagging and
Boosting</a></p></li><li><p>To explore more than is covered in the book on gradient boosting you
can start at the <a href=https://en.wikipedia.org/wiki/Gradient_boosting rel=external target=_blank>Wikipedia
page</a> and follow
the references.</p></li><li><p>For your project you should explore scikit-learn
<a href=https://scikit-learn.org/stable/modules/ensemble.html rel=external target=_blank>API</a> on
ensemble methods.</p></li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>How do I use decision trees for regression?</li><li>What is ensemble learning?</li><li>How do random forest work?</li><li>What is boosting and bagging?</li><li>What is gradient boosting?</li></ul><footer class=footline></footer></article></div></main></div><script src=/js/clipboard.min.js?1737535569 defer></script><script src=/js/perfect-scrollbar.min.js?1737535569 defer></script><script src=/js/theme.js?1737535569 defer></script></body></html>