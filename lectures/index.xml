<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lectures on Introduction to ML 1D034 homepage.</title>
    <link>https://intro-ml-1dl034-uu-se.github.io/lectures/index.html</link>
    <description>Recent content in Lectures on Introduction to ML 1D034 homepage.</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>justin.pearson@it.uu.se (Justin Pearson)</managingEditor>
    <webMaster>justin.pearson@it.uu.se (Justin Pearson)</webMaster>
    <atom:link href="https://intro-ml-1dl034-uu-se.github.io/lectures/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lecture 1: Introduction and Overview of the Course</title>
      <link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture1/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author>
      <guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture1/index.html</guid>
      <description>Today&amp;rsquo;s Topic Overview of the course and introduction to machine learning. What is supervised learning and what is unsupervised learning.
Link to the slides. I use these slides.
Reading Guide The Hundred-Page Machine Learning Book Chapter 1, Chapter 2 contains some background mathematics and some definitions that you should know. What should I know by the end of this lecture? How is the course organised? What is machine learning? What is supervised learning?</description>
    </item>
    <item>
      <title>Lecture 2: Linear Regression as Machine Learning</title>
      <link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture2/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author>
      <guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture2/index.html</guid>
      <description>Today&amp;rsquo;s Topics Linear Regression Linear Regression as a machine learning algorithm. Machine learning algorithms and hypothesises. In short a machine learning find the best hypothesis that explains that data. A cost function (or an error function, or a loss function) measure how far way a hypothesis is from explaining the data: the smaller the cost, the better the hypothesis.
Ideally you want an algorithm takes the training data and gives you the hypothesis that with the smallest cost value.</description>
    </item>
    <item>
      <title>Lecture 3: Probability and Naive Bayes Classification</title>
      <link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture3/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author>
      <guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture3/index.html</guid>
      <description>Today&amp;rsquo;s Topic Using Bayes&amp;rsquo; theorem for machine learning. You should do some revision on the use of Bayes&amp;rsquo; theorem in general. In this lecture you will look at how to use Bayes&amp;rsquo; theorem to build a spam detector. One important idea to take away from this lecture is that there are a variety of ways implementing spam detection: in particular there are different feature models that you can use that give you different ways of calculating the relevant probabilities.</description>
    </item>
    <item>
      <title>Lecture 4: Logistic Regression as Machine Learning</title>
      <link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture4/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author>
      <guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture4/index.html</guid>
      <description>Today&amp;rsquo;s Topics Today&amp;rsquo;s slides.
Logistic Regression Logistic regression, overfitting and regularisation. Again Logistic regression is an algorithm that comes from statistics, but it can also seen as a machine learning algorithm. The hypothesis is very similar to linear regression is it a set of values that defines a linear function. The difference between logistic regression and linear regression is the linear function goes through a logistic function that works as a threshold function.</description>
    </item>
    <item>
      <title>Lecture 5: Support Vector Machines</title>
      <link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture5/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author>
      <guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture5/index.html</guid>
      <description>Today&amp;rsquo;s topic Support Vector Machines (SVM) Support vector machines (SVMs) are used for classification, and use some of the ideas from logistic regression. Support vector machines deal with noisy data, where some labels are miss-classified, with large-margin classifiers.
Support vector machines can also do non-linear classification using kernels. A kernel is a non-linear transformation of the input data into a higher dimensional space. Kernels transform your input data into a space where it is possible to do linear separation as in logistic regression.</description>
    </item>
    <item>
      <title>Lecture 6: Cross Validation and Feature Engineering</title>
      <link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture6/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author>
      <guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture6/index.html</guid>
      <description>Today&amp;rsquo;s Topics In today&amp;rsquo;s lecture you will look at various techniques to deal with and understand overfitting. Dealing with overfitting leads nicely to the model selection problem. First and foremost, how do you decide which machine learning algorithm to use. Further, in many machine learning algorithms there are hyper-parameters that are not decided by the training data. Choosing which model to use or values of the models hyper-parameters is a difficult task and can greatly affect the performance of you algorithm.</description>
    </item>
    <item>
      <title>Lecture 7: Clustering and Nearest Neighbours</title>
      <link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture7/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author>
      <guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture7/index.html</guid>
      <description>Today&amp;rsquo;s Topics In this segment you are looking at some unsupervised algorithms, as well as one supervised learning method (K-nearest neighbours) The main unsupervised algorithms are Hierarchical clustering, K-means clustering and DBSCAN. It is important not to get K-nearest neighbours and K-means clustering confused.
The K-means algorithm works by gradient descent. Unlike a lot of the algorithms that we have been looking at, K-means often suffers from the problem of many local minima.</description>
    </item>
    <item>
      <title>Lecture 8: Decision Trees</title>
      <link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture8/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author>
      <guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture8/index.html</guid>
      <description>Today&amp;rsquo;s Topics Decision trees are another powerful machine learning technique, and has the advantage that it is easy to see what the algorithm has learned. Constructing the optimal decision tree for an arbitrary data set is yet another NP-hard question, and there are many algorithms available. In this lecture you will look at one of the simpler algorithm ID3. ID3 uses information theory to decide how to construct a tree.</description>
    </item>
    <item>
      <title>Lecture 9: Principle Component Analysis and Preprocessing</title>
      <link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture9/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author>
      <guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture9/index.html</guid>
      <description>Today&amp;rsquo;s Topics As we have seen before it is often a good idea to do some pre-processing of your input data. Sometimes there are linear relationships hidden in the data, and if you do not remove or discover them then your machine learning algorithm will be trying to learn these linear relationships. In linear algebra, principle component analysis (PCA) is a well established method of reducing the dimension of a data set.</description>
    </item>
    <item>
      <title>Lecture 10: Ensemble Learning</title>
      <link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture10/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author>
      <guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture10/index.html</guid>
      <description>Slides Link for the slides.
Today&amp;rsquo;s Topics: Ensemble Learning Ensemble learning is a simple idea. Instead of training one model, we train multiple models and combine the results. A popular ensemble learning algorithm is random forests that combines many decision trees that are trained on random subsets of the features. To build a classifier a majority vote is taken. There are various techniques to improve the system including boosting, bagging and gradient boosting.</description>
    </item>
  </channel>
</rss>