<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Lectures :: 1DL034</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/index.html</link><description>Lecture plan, timetable and link to slides.</description><generator>Hugo</generator><language>en-uk</language><managingEditor>justin.pearson@it.uu.se (Justin Pearson)</managingEditor><webMaster>justin.pearson@it.uu.se (Justin Pearson)</webMaster><atom:link href="https://intro-ml-1dl034-uu-se.github.io/lectures/index.xml" rel="self" type="application/rss+xml"/><item><title>Lecture 1: Introduction and Overview of the Course</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture1/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture1/index.html</guid><description>Today’s Topic Overview of the course and introduction to machine learning. What is supervised learning and what is unsupervised learning.
Link to the slides. I use these slides.
Reading Guide The Hundred-Page Machine Learning Book Chapter 1, Chapter 2 contains some background mathematics and some definitions that you should know. Chapter 1 and 2 of A Hands-On Introduction to Machine Learning What should I know by the end of this lecture? How is the course organised? What is machine learning? What is supervised learning? What is unsupervised learning?</description></item><item><title>Lecture 2: Linear Regression as Machine Learning</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture2/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture2/index.html</guid><description>Today’s Topics Linear Regression Linear Regression as a machine learning algorithm. Machine learning algorithms and hypothesises. In short a machine learning find the best hypothesis that explains that data. A cost function (or an error function, or a loss function) measure how far way a hypothesis is from explaining the data: the smaller the cost, the better the hypothesis.</description></item><item><title>Lecture 3: Probability and Naive Bayes Classification</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture3/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture3/index.html</guid><description>Today’s Topic Using Bayes’ theorem for machine learning. You should do some revision on the use of Bayes’ theorem in general. In this lecture you will look at how to use Bayes’ theorem to build a spam detector. One important idea to take away from this lecture is that there are a variety of ways implementing spam detection: in particular there are different feature models that you can use that give you different ways of calculating the relevant probabilities. It is important that you understand the difference between the different ways of implementing spam detection.</description></item><item><title>Lecture 4: Logistic Regression as Machine Learning</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture4/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture4/index.html</guid><description>Today’s Topics Today’s slides.
Logistic Regression Logistic regression, overfitting and regularisation. Again Logistic regression is an algorithm that comes from statistics, but it can also seen as a machine learning algorithm. The hypothesis is very similar to linear regression is it a set of values that defines a linear function. The difference between logistic regression and linear regression is the linear function goes through a logistic function that works as a threshold function. Unlike linear regression it is not possible to solve the model exactly, and gradient descent is necessary.</description></item><item><title>Lecture 5: Support Vector Machines</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture5/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture5/index.html</guid><description>Today’s topic Support Vector Machines (SVM) Support vector machines (SVMs) are used for classification, and use some of the ideas from logistic regression. Support vector machines deal with noisy data, where some labels are miss-classified, with large-margin classifiers.
Support vector machines can also do non-linear classification using kernels. A kernel is a non-linear transformation of the input data into a higher dimensional space. Kernels transform your input data into a space where it is possible to do linear separation as in logistic regression.</description></item><item><title>Lecture 6: Cross Validation and Feature Engineering</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture6/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture6/index.html</guid><description>Today’s Topics In today’s lecture you will look at various techniques to deal with and understand overfitting. Dealing with overfitting leads nicely to the model selection problem. First and foremost, how do you decide which machine learning algorithm to use. Further, in many machine learning algorithms there are hyper-parameters that are not decided by the training data. Choosing which model to use or values of the models hyper-parameters is a difficult task and can greatly affect the performance of you algorithm. Cross validation is a useful technique from statistics that allows you to partition your data up into many combinations of training, test and validation sets. You can then use cross validation to help you decide which machine learning model to use and what values to set the hyper-parameters.</description></item><item><title>Lecture 7: Clustering and Nearest Neighbours</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture7/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture7/index.html</guid><description>Today’s Topics In this segment you are looking at some unsupervised algorithms, as well as one supervised learning method (K-nearest neighbours) The main unsupervised algorithms are Hierarchical clustering, K-means clustering and DBSCAN. It is important not to get K-nearest neighbours and K-means clustering confused.
The K-means algorithm works by gradient descent. Unlike a lot of the algorithms that we have been looking at, K-means often suffers from the problem of many local minima. In Andrew Ng’s lectures you will meet various ways of dealing with local minima.</description></item><item><title>Lecture 8: Decision Trees</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture8/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture8/index.html</guid><description>Today’s Topics Decision trees are another powerful machine learning technique, and has the advantage that it is easy to see what the algorithm has learned. Constructing the optimal decision tree for an arbitrary data set is yet another NP-hard question, and there are many algorithms available. In this lecture you will look at one of the simpler algorithm ID3. ID3 uses information theory to decide how to construct a tree.</description></item><item><title>Lecture 9: Principle Component Analysis and Preprocessing</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture9/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture9/index.html</guid><description>Today’s Topics As we have seen before it is often a good idea to do some pre-processing of your input data. Sometimes there are linear relationships hidden in the data, and if you do not remove or discover them then your machine learning algorithm will be trying to learn these linear relationships. In linear algebra, principle component analysis (PCA) is a well established method of reducing the dimension of a data set. It uses eigenvalues and eigenvectors to uncover hidden linear relationships in your data-set. For this course you do not need to know how to actually compute eigenvalues and eigenvectors by hand, but you should understand the definition and what is going on.</description></item><item><title>Lecture 10: Ensemble Learning</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture10/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture10/index.html</guid><description>Slides Link for the slides.
Today’s Topics: Ensemble Learning Ensemble learning is a simple idea. Instead of training one model, we train multiple models and combine the results. A popular ensemble learning algorithm is random forests that combines many decision trees that are trained on random subsets of the features. To build a classifier a majority vote is taken. There are various techniques to improve the system including boosting, bagging and gradient boosting. As you will see below these are not limited to random forests, but to other combinations of learning algorithms. There is a lot extensions and refinements of ensemble methods including AdaBoost. We will only cover gradient boosting and not go too much into the mathematics behind ensemble learning (that is reserved for a more advanced course). The aim of this lecture is to give you access to another powerful machine learning technique.</description></item><item><title>Lecture 11: Ethics and Bias</title><link>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture11/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>justin.pearson@it.uu.se (Justin Pearson)</author><guid>https://intro-ml-1dl034-uu-se.github.io/lectures/lecture11/index.html</guid><description>Slides Link for the slides.
Today’s Topics: Ethics, Bias and the Law Introduction to ethics in AI and computer science Definitions and introduction to bias. Some technical discussions on how to avoid bias. Introduction to EU laws on AI. Reading Guide There are a lot of links in the slides (email me if they are broken). You are encouraged to read them. The whole EU AI Act is long and written for lawyers. You should read Ethics guidelines for trustworthy AI instead.</description></item></channel></rss>