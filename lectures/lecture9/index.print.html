<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.123.3">
    <meta name="generator" content="Relearn 5.24.3+tip">
    <meta name="description" content="">
    <meta name="author" content="Justin Pearson">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Lecture 9: Principle Component Analysis and Preprocessing :: Introduction to ML 1D034 homepage.">
    <meta name="twitter:description" content="Today&rsquo;s Topics As we have seen before it is often a good idea to do some pre-processing of your input data. Sometimes there are linear relationships hidden in the data, and if you do not remove or discover them then your machine learning algorithm will be trying to learn these linear relationships. In linear algebra, principle component analysis (PCA) is a well established method of reducing the dimension of a data set.">
    <meta property="og:title" content="Lecture 9: Principle Component Analysis and Preprocessing :: Introduction to ML 1D034 homepage.">
    <meta property="og:description" content="Today&rsquo;s Topics As we have seen before it is often a good idea to do some pre-processing of your input data. Sometimes there are linear relationships hidden in the data, and if you do not remove or discover them then your machine learning algorithm will be trying to learn these linear relationships. In linear algebra, principle component analysis (PCA) is a well established method of reducing the dimension of a data set.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://intro-ml-1dl034-uu-se.github.io/lectures/lecture9/index.html">
    <meta property="article:section" content="Lectures :: Introduction to ML 1D034 homepage.">
    <meta property="og:site_name" content="Introduction to ML 1D034 homepage.">
    <title>Lecture 9: Principle Component Analysis and Preprocessing :: Introduction to ML 1D034 homepage.</title>
    <link href="https://intro-ml-1dl034-uu-se.github.io/lectures/lecture9/index.html" rel="canonical" type="text/html" title="Lecture 9: Principle Component Analysis and Preprocessing :: Introduction to ML 1D034 homepage.">
    <link href="/lectures/lecture9/index.xml" rel="alternate" type="application/rss+xml" title="Lecture 9: Principle Component Analysis and Preprocessing :: Introduction to ML 1D034 homepage.">
    <!-- https://github.com/filamentgroup/loadCSS/blob/master/README.md#how-to-use -->
    <link href="/css/fontawesome-all.min.css?1709398718" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/fontawesome-all.min.css?1709398718" rel="stylesheet"></noscript>
    <link href="/css/nucleus.css?1709398718" rel="stylesheet">
    <link href="/css/auto-complete.css?1709398718" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/auto-complete.css?1709398718" rel="stylesheet"></noscript>
    <link href="/css/perfect-scrollbar.min.css?1709398718" rel="stylesheet">
    <link href="/css/fonts.css?1709398718" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/fonts.css?1709398718" rel="stylesheet"></noscript>
    <link href="/css/theme.css?1709398718" rel="stylesheet">
    <link href="/css/theme-blue.css?1709398718" rel="stylesheet" id="R-variant-style">
    <link href="/css/chroma-learn.css?1709398718" rel="stylesheet" id="R-variant-chroma-style">
    <link href="/css/variant.css?1709398718" rel="stylesheet">
    <link href="/css/print.css?1709398718" rel="stylesheet" media="print">
    <link href="/css/format-print.css?1709398718" rel="stylesheet">
    <link href="/css/ie.css?1709398718" rel="stylesheet">
    <script src="/js/url.js?1709398718"></script>
    <script src="/js/variant.js?1709398718"></script>
    <script>
      // hack to let hugo tell us how to get to the root when using relativeURLs, it needs to be called *url= for it to do its magic:
      // https://github.com/gohugoio/hugo/blob/145b3fcce35fbac25c7033c91c1b7ae6d1179da8/transform/urlreplacers/absurlreplacer.go#L72
      window.index_js_url="/index.search.js";
      var root_url="/";
      var baseUri=root_url.replace(/\/$/, '');
      window.relearn = window.relearn || {};
      window.relearn.baseUriFull='https:\/\/intro-ml-1dl034-uu-se.github.io/';
      // variant stuff
      window.relearn.themeVariantModifier='';
      window.variants && variants.init( [ 'blue' ] );
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
    </script>
  </head>
  <body class="mobile-support print" data-url="/lectures/lecture9/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="/index.html"><span itemprop="name">Introduction to Machine Learning 1DL034</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="/lectures/index.html"><span itemprop="name">Lectures</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><span itemprop="name">Lecture 9: Principle Component Analysis and Preprocessing</span><meta itemprop="position" content="3"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable default" tabindex="-1">
        <div class="flex-block-wrapper">
          <article class="default">
            <header class="headline">
            </header>
<h1 id="lecture-9-principle-component-analysis-and-preprocessing">Lecture 9: Principle Component Analysis and Preprocessing</h1>

<h2 id="todays-topics">Today&rsquo;s Topics</h2>
<p>As we have seen before it is often a good idea to do some
pre-processing of your input data. Sometimes there are linear
relationships hidden in the data, and if you do not remove or discover
them then your machine learning algorithm will be trying to learn
these linear relationships. In linear algebra, principle component
analysis (PCA) is a well established method of reducing the dimension
of a data set. It uses eigenvalues and eigenvectors to uncover hidden
linear relationships in your data-set. For this course you do not need
to know how to actually compute eigenvalues and eigenvectors by hand,
but you should understand the definition and what is going on.</p>
<p>An interesting application of PCA are
<a href="https://en.wikipedia.org/wiki/Eigenface" target="_blank">Eigenfaces</a>.</p>
<h2 id="slides">Slides</h2>
<p>I use <a href="https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture9.pdf" target="_blank">these
slide</a>. In
previous years, I have done the lecture on the blackboard, and these
are some
<a href="https://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture9_handwritten.pdf" target="_blank">notes</a>
that I used. They contain some extra derivations.</p>
<h2 id="reading-guide">Reading Guide</h2>
<ul>
<li><a href="http://themlbook.com/" target="_blank">Hundred-Page Machine Learning Book</a>
<a href="https://www.dropbox.com/s/y9a7b0hzmuksqar/Chapter9.pdf?dl=0" target="_blank">Chapter  9 section 9.3
</a></li>
<li><a href="https://uub.primo.exlibrisgroup.com/permalink/46LIBRIS_UUB/d23b4h/alma991018444332907596" target="_blank">A First Course in Machine
Learning</a> 7.1,7.2 has a good
derivation of PCA from the point of view of minimising the
variance in the data.</li>
<li><a href="https://medium.com/apprentice-journal/pca-application-in-machine-learning-4827c07a61db" target="_blank">PCA in Machine
Learning</a>
<ul>
<li><a href="https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html" target="_blank">Principal Component
Analysis</a></li>
<li><a href="https://towardsdatascience.com/pca-eigenvectors-and-eigenvalues-1f968bc6777a" target="_blank">Towards Data Science on PCA</a></li>
</ul>
</li>
</ul>
<h2 id="what-should-i-know-by-the-end-of-this-lecture">What should I know by the end of this lecture?</h2>
<ul>
<li>What is principle competent analysis (PCA)?</li>
<li>What is the co-variance matrix and what do the entries mean?</li>
<li>What do the eigenvalues of the co-variance matrix tell you about the
data?</li>
<li>How do you choose the number of dimensions in PCA?</li>
<li>What are some applications of PCA in machine learning?</li>
</ul>

            <footer class="footline">
            </footer>
          </article>

        </div>
      </main>
    </div>
    <script src="/js/clipboard.min.js?1709398718" defer></script>
    <script src="/js/perfect-scrollbar.min.js?1709398718" defer></script>
    <script src="/js/theme.js?1709398718" defer></script>
  </body>
</html>
