<!doctype html><html lang=en dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=print><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.140.0"><meta name=generator content="Relearn 7.3.1"><meta name=description content="Today’s Topics Decision trees are another powerful machine learning technique, and has the advantage that it is easy to see what the algorithm has learned. Constructing the optimal decision tree for an arbitrary data set is yet another NP-hard question, and there are many algorithms available. In this lecture you will look at one of the simpler algorithm ID3. ID3 uses information theory to decide how to construct a tree."><meta name=author content="Justin Pearson"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lecture 8: Decision Trees :: 1DL034"><meta name=twitter:description content="Today’s Topics Decision trees are another powerful machine learning technique, and has the advantage that it is easy to see what the algorithm has learned. Constructing the optimal decision tree for an arbitrary data set is yet another NP-hard question, and there are many algorithms available. In this lecture you will look at one of the simpler algorithm ID3. ID3 uses information theory to decide how to construct a tree."><meta property="og:url" content="https://intro-ml-1dl034-uu-se.github.io/lectures/lecture8/index.html"><meta property="og:site_name" content="1DL034"><meta property="og:title" content="Lecture 8: Decision Trees :: 1DL034"><meta property="og:description" content="Today’s Topics Decision trees are another powerful machine learning technique, and has the advantage that it is easy to see what the algorithm has learned. Constructing the optimal decision tree for an arbitrary data set is yet another NP-hard question, and there are many algorithms available. In this lecture you will look at one of the simpler algorithm ID3. ID3 uses information theory to decide how to construct a tree."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="Lectures"><meta itemprop=name content="Lecture 8: Decision Trees :: 1DL034"><meta itemprop=description content="Today’s Topics Decision trees are another powerful machine learning technique, and has the advantage that it is easy to see what the algorithm has learned. Constructing the optimal decision tree for an arbitrary data set is yet another NP-hard question, and there are many algorithms available. In this lecture you will look at one of the simpler algorithm ID3. ID3 uses information theory to decide how to construct a tree."><meta itemprop=wordCount content="202"><title>Lecture 8: Decision Trees :: 1DL034</title>
<link href=https://intro-ml-1dl034-uu-se.github.io/lectures/lecture8/index.html rel=canonical type=text/html title="Lecture 8: Decision Trees :: 1DL034"><link href=/lectures/lecture8/index.xml rel=alternate type=application/rss+xml title="Lecture 8: Decision Trees :: 1DL034"><link href=/css/fontawesome-all.min.css?1737535569 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fontawesome-all.min.css?1737535569 rel=stylesheet></noscript><link href=/css/auto-complete.css?1737535569 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/auto-complete.css?1737535569 rel=stylesheet></noscript><link href=/css/perfect-scrollbar.min.css?1737535569 rel=stylesheet><link href=/css/theme.min.css?1737535569 rel=stylesheet><link href=/css/format-print.min.css?1737535569 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.relBasePath="../..",window.relearn.relBaseUri="../..",window.relearn.absBaseUri="https://intro-ml-1dl034-uu-se.github.io",window.relearn.min=`.min`,window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.themevariants=["blue"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}}))},window.relearn.markVariant=function(){var t=window.localStorage.getItem(window.relearn.absBaseUri+"/variant"),e=document.querySelector("#R-select-variant");e&&(e.value=t)},window.relearn.initVariant=function(){var e=window.localStorage.getItem(window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant(),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`</script></head><body class="mobile-support print" data-url=/lectures/lecture8/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><button class=topbar-control onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/index.html><span itemprop=name>Introduction to Machine Learning 1DL034</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/lectures/index.html><span itemprop=name>Lectures</span></a><meta itemprop=position content="2">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>Lecture 8</span><meta itemprop=position content="3"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-print" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><a class=topbar-control href=/lectures/lecture8/index.print.html title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable lectures" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=lecture-8-decision-trees>Lecture 8: Decision Trees</h1><h2 id=todays-topics>Today&rsquo;s Topics</h2><p>Decision trees are another powerful machine learning technique, and
has the advantage that it is easy to see what the algorithm has
learned. Constructing the optimal decision tree for an arbitrary data
set is yet another NP-hard question, and there are many algorithms
available. In this lecture you will look at one of the simpler
algorithm ID3. ID3 uses information theory to decide how to construct
a tree.</p><h2 id=slides-and-notebooks>Slides and Notebooks</h2><ul><li>Lecture
<a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture8.pdf rel=external target=_blank>slides</a></li><li><a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Notebooks/pandas_id3.ipynb rel=external target=_blank>Python
notebook</a>
used to prepare the lecture. You can view it <a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Notebooks/pandas_id3.html rel=external target=_blank>here</a></li></ul><h2 id=reading-guide>Reading Guide</h2><h3 id=information-theory>Information theory.</h3><p>You do not need to know much about information theory to apply the
ID3 algorithm, but the first five chapters of
<strong>An introduction to information theory: symbols, signals & noise</strong>
by Pierce, John R will give some
background to people who are interested. The book is available
online at the <a href=http://www.ub.uu.se rel=external target=_blank>University library</a>.</p><h3 id=decision-trees>Decision Trees</h3><ul><li><p><a href=http://themlbook.com/ rel=external target=_blank>The Hundred-Page Machine Learning Book</a>
<a href="https://www.dropbox.com/s/qiq2c85cle9ydb6/Chapter3.pdf?dl=0" rel=external target=_blank>Section 3.3</a></p></li><li><p>See the excellent <a href=http://www.cse.chalmers.se/~richajo/dit866/lectures/l1/decision_trees.pdf rel=external target=_blank>notes</a> by Richard Johansson from Chalmers</p></li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>What are decision trees?</li><li>What is Shannon entropy and how does it measure information?</li><li>How do I calculate the Shannon entropy of a distribution?</li><li>How does the ID3 learning algorithm work?</li></ul><footer class=footline></footer></article></div></main></div><script src=/js/clipboard.min.js?1737535569 defer></script><script src=/js/perfect-scrollbar.min.js?1737535569 defer></script><script src=/js/theme.js?1737535569 defer></script></body></html>