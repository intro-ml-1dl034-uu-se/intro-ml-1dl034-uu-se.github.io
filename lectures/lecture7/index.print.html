<!doctype html><html lang=en dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=print><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.145.0"><meta name=generator content="Relearn 7.3.1"><meta name=description content="Today’s Topics In this segment you are looking at some unsupervised algorithms, as well as one supervised learning method (K-nearest neighbours) The main unsupervised algorithms are Hierarchical clustering, K-means clustering and DBSCAN. It is important not to get K-nearest neighbours and K-means clustering confused.
The K-means algorithm works by gradient descent. Unlike a lot of the algorithms that we have been looking at, K-means often suffers from the problem of many local minima. In Andrew Ng’s lectures you will meet various ways of dealing with local minima."><meta name=author content="Justin Pearson"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lecture 7: Clustering and Nearest Neighbours :: 1DL034"><meta name=twitter:description content="Today’s Topics In this segment you are looking at some unsupervised algorithms, as well as one supervised learning method (K-nearest neighbours) The main unsupervised algorithms are Hierarchical clustering, K-means clustering and DBSCAN. It is important not to get K-nearest neighbours and K-means clustering confused.
The K-means algorithm works by gradient descent. Unlike a lot of the algorithms that we have been looking at, K-means often suffers from the problem of many local minima. In Andrew Ng’s lectures you will meet various ways of dealing with local minima."><meta property="og:url" content="https://intro-ml-1dl034-uu-se.github.io/lectures/lecture7/index.html"><meta property="og:site_name" content="1DL034"><meta property="og:title" content="Lecture 7: Clustering and Nearest Neighbours :: 1DL034"><meta property="og:description" content="Today’s Topics In this segment you are looking at some unsupervised algorithms, as well as one supervised learning method (K-nearest neighbours) The main unsupervised algorithms are Hierarchical clustering, K-means clustering and DBSCAN. It is important not to get K-nearest neighbours and K-means clustering confused.
The K-means algorithm works by gradient descent. Unlike a lot of the algorithms that we have been looking at, K-means often suffers from the problem of many local minima. In Andrew Ng’s lectures you will meet various ways of dealing with local minima."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="Lectures"><meta itemprop=name content="Lecture 7: Clustering and Nearest Neighbours :: 1DL034"><meta itemprop=description content="Today’s Topics In this segment you are looking at some unsupervised algorithms, as well as one supervised learning method (K-nearest neighbours) The main unsupervised algorithms are Hierarchical clustering, K-means clustering and DBSCAN. It is important not to get K-nearest neighbours and K-means clustering confused.
The K-means algorithm works by gradient descent. Unlike a lot of the algorithms that we have been looking at, K-means often suffers from the problem of many local minima. In Andrew Ng’s lectures you will meet various ways of dealing with local minima."><meta itemprop=wordCount content="353"><title>Lecture 7: Clustering and Nearest Neighbours :: 1DL034</title>
<link href=https://intro-ml-1dl034-uu-se.github.io/lectures/lecture7/index.html rel=canonical type=text/html title="Lecture 7: Clustering and Nearest Neighbours :: 1DL034"><link href=/lectures/lecture7/index.xml rel=alternate type=application/rss+xml title="Lecture 7: Clustering and Nearest Neighbours :: 1DL034"><link href=/css/fontawesome-all.min.css?1740731316 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fontawesome-all.min.css?1740731316 rel=stylesheet></noscript><link href=/css/auto-complete.css?1740731316 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/auto-complete.css?1740731316 rel=stylesheet></noscript><link href=/css/perfect-scrollbar.min.css?1740731316 rel=stylesheet><link href=/css/theme.min.css?1740731316 rel=stylesheet><link href=/css/format-print.min.css?1740731316 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.relBasePath="../..",window.relearn.relBaseUri="../..",window.relearn.absBaseUri="https://intro-ml-1dl034-uu-se.github.io",window.relearn.min=`.min`,window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.themevariants=["blue"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}}))},window.relearn.markVariant=function(){var t=window.localStorage.getItem(window.relearn.absBaseUri+"/variant"),e=document.querySelector("#R-select-variant");e&&(e.value=t)},window.relearn.initVariant=function(){var e=window.localStorage.getItem(window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant(),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`</script></head><body class="mobile-support print" data-url=/lectures/lecture7/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><button class=topbar-control onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/index.html><span itemprop=name>Introduction to Machine Learning 1DL034</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/lectures/index.html><span itemprop=name>Lectures</span></a><meta itemprop=position content="2">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>Lecture 7</span><meta itemprop=position content="3"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-print" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><a class=topbar-control href=/lectures/lecture7/index.print.html title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable lectures" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=lecture-7-clustering-and-nearest-neighbours>Lecture 7: Clustering and Nearest Neighbours</h1><h2 id=todays-topics>Today&rsquo;s Topics</h2><p>In this segment you are looking at some unsupervised algorithms, as
well as one supervised learning method (K-nearest neighbours) The
main unsupervised algorithms are Hierarchical clustering, K-means
clustering and DBSCAN. It is important not to get K-nearest neighbours
and K-means clustering confused.</p><p>The K-means algorithm works by gradient descent. Unlike a lot of the
algorithms that we have been looking at, K-means often suffers from
the problem of many local minima. In Andrew Ng&rsquo;s lectures you will
meet various ways of dealing with local minima.</p><p>If you have taken <a href=https://ad2-uu-se.github.io/ rel=external target=_blank>Algorithms and Data Structures II</a>(<a href="https://www.uu.se/en/admissions/master/selma/kursplan/?kKod=1DL231" rel=external target=_blank>1DL231</a>)
or AD3
(<a href="https://www.uu.se/en/admissions/master/selma/kursplan/?kKod=1DL481" rel=external target=_blank>1DL481</a>),
then you will have met the concept of <a href=https://ad2-uu-se.github.io/lectures/lectures12-13/index.html rel=external target=_blank>NP-hardness</a>. K-means clustering is NP-hard
(see the reference below). This means that the problem is not easy to
solve. If you could guarantee that there would only be one global
minimum then gradient descent would be an efficient algorithm. This
implies that there will always often be local minima in K-means
clustering.</p><h2 id=slides>Slides</h2><p>I used these
<a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture7.pdf rel=external target=_blank>slides</a>
in the lecture.</p><h2 id=reading-guide>Reading Guide</h2><h3 id=k-means-clustering>K-Means Clustering</h3><ul><li><a href=http://themlbook.com/ rel=external target=_blank>Hundred-Page Machine Learning Book</a> <a href="https://www.dropbox.com/s/qiq2c85cle9ydb6/Chapter3.pdf?dl=0" rel=external target=_blank>Chapter
3</a>
section 3.5 and <a href="https://www.dropbox.com/s/y9a7b0hzmuksqar/Chapter9.pdf?dl=0" rel=external target=_blank>Chapter
9</a> all
of 9.2</li><li>Chapter 7 of <a href=https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview rel=external target=_blank>A Hands-On Introduction to Machine
Learning</a>.</li><li>Chapter 6 (6.1 and 6.2) of <a href=https://tinyurl.com/y2obtldw rel=external target=_blank>A First Course in Machine
Learning</a>. The link takes you to the
electronic copy in the library.</li><li>This is only for reference: <a href=https://cseweb.ucsd.edu/~avattani/papers/kmeans_hardness.pdf rel=external target=_blank>NP Hardness of K-means
clustering</a>. You
don&rsquo;t need to understand the proof, although you should be aware of
its implications. No greedy/gradient descent algorithm for K-means
is going to be exact.</li></ul><h3 id=k-nearest-neighbours>K-Nearest Neighbours</h3><ul><li>The <a href=https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm rel=external target=_blank>Wikipedia page on K-Nearest
Neighbours</a>
is a good starting point.</li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?</h2><ul><li>What are some of the applications of clustering?</li><li>What is hierarchical clustering and what algorithms are there?</li><li>How does the K-means algorithm work? What is the cost function?</li><li>What is a local optima and why is it a problem with the K-means
algorithm?</li><li>What are some approaches to choosing the number of clusters in
K-means?</li><li>How does the K-nearest neighbour algorithm work and what are some of
its applications?</li><li>What is DBSCAN and how does it work?</li></ul><footer class=footline></footer></article></div></main></div><script src=/js/clipboard.min.js?1740731316 defer></script><script src=/js/perfect-scrollbar.min.js?1740731316 defer></script><script src=/js/theme.js?1740731316 defer></script></body></html>