<!doctype html><html lang=en-uk dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=print><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.154.5"><meta name=generator content="Relearn 8.3.0+6dafca9f9e6c9639c4a2b886e097505d5ee31955"><meta name=description content="Today’s Topics In this segment you are looking at some unsupervised algorithms, as well as one supervised learning method (K-nearest neighbours) The main unsupervised algorithms are Hierarchical clustering, K-means clustering and DBSCAN. It is important not to get K-nearest neighbours and K-means clustering confused.
The K-means algorithm works by gradient descent. Unlike a lot of the algorithms that we have been looking at, K-means often suffers from the problem of many local minima. In Andrew Ng’s lectures you will meet various ways of dealing with local minima."><meta name=author content="Justin Pearson"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lecture 7: Clustering and Nearest Neighbours :: 1DL034"><meta name=twitter:description content="Today’s Topics In this segment you are looking at some unsupervised algorithms, as well as one supervised learning method (K-nearest neighbours) The main unsupervised algorithms are Hierarchical clustering, K-means clustering and DBSCAN. It is important not to get K-nearest neighbours and K-means clustering confused.
The K-means algorithm works by gradient descent. Unlike a lot of the algorithms that we have been looking at, K-means often suffers from the problem of many local minima. In Andrew Ng’s lectures you will meet various ways of dealing with local minima."><meta property="og:url" content="https://intro-ml-1dl034-uu-se.github.io/lectures/lecture7/index.html"><meta property="og:site_name" content="1DL034"><meta property="og:title" content="Lecture 7: Clustering and Nearest Neighbours :: 1DL034"><meta property="og:description" content="Today’s Topics In this segment you are looking at some unsupervised algorithms, as well as one supervised learning method (K-nearest neighbours) The main unsupervised algorithms are Hierarchical clustering, K-means clustering and DBSCAN. It is important not to get K-nearest neighbours and K-means clustering confused.
The K-means algorithm works by gradient descent. Unlike a lot of the algorithms that we have been looking at, K-means often suffers from the problem of many local minima. In Andrew Ng’s lectures you will meet various ways of dealing with local minima."><meta property="og:locale" content="en_uk"><meta property="og:type" content="article"><meta property="article:section" content="Lectures"><meta itemprop=name content="Lecture 7: Clustering and Nearest Neighbours :: 1DL034"><meta itemprop=description content="Today’s Topics In this segment you are looking at some unsupervised algorithms, as well as one supervised learning method (K-nearest neighbours) The main unsupervised algorithms are Hierarchical clustering, K-means clustering and DBSCAN. It is important not to get K-nearest neighbours and K-means clustering confused.
The K-means algorithm works by gradient descent. Unlike a lot of the algorithms that we have been looking at, K-means often suffers from the problem of many local minima. In Andrew Ng’s lectures you will meet various ways of dealing with local minima."><meta itemprop=wordCount content="353"><title>Lecture 7: Clustering and Nearest Neighbours :: 1DL034</title><link href=https://intro-ml-1dl034-uu-se.github.io/lectures/lecture7/index.html rel=canonical type=text/html title="Lecture 7: Clustering and Nearest Neighbours :: 1DL034"><link href=/lectures/lecture7/index.xml rel=alternate type=application/rss+xml title="Lecture 7: Clustering and Nearest Neighbours :: 1DL034"><link href=/css/auto-complete/auto-complete.min.css?1769686469 rel=stylesheet><script src=/js/auto-complete/auto-complete.min.js?1769686469 defer></script><script src=/js/search-lunr.min.js?1769686469 defer></script><script src=/js/search.min.js?1769686469 defer></script><script>window.relearn=window.relearn||{},window.relearn.index_js_url="/searchindex.en.js?1769686469"</script><script src=/js/lunr/lunr.min.js?1769686469 defer></script><script src=/js/lunr/lunr.stemmer.support.min.js?1769686469 defer></script><script src=/js/lunr/lunr.multi.min.js?1769686469 defer></script><script src=/js/lunr/lunr.en.min.js?1769686469 defer></script><script>window.relearn=window.relearn||{},window.relearn.contentLangs=["en"]</script><link href=/fonts/fontawesome/css/fontawesome-all.min.css?1769686469 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/fonts/fontawesome/css/fontawesome-all.min.css?1769686469 rel=stylesheet></noscript><link href=/css/perfect-scrollbar/perfect-scrollbar.min.css?1769686469 rel=stylesheet><link href=/css/theme.min.css?1769686469 rel=stylesheet><link href=/css/format-print.min.css?1769686469 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.min=`.min`,window.relearn.path="/lectures/lecture7/index.html",window.relearn.relBasePath="../..",window.relearn.relBaseUri="../..",window.relearn.absBaseUri="https://intro-ml-1dl034-uu-se.github.io",window.relearn.disableInlineCopyToClipboard=!1,window.relearn.enableBlockCodeWrap=!0,window.relearn.getItem=(e,t)=>e.getItem(t),window.relearn.setItem=(e,t,n)=>e.setItem(t,n),window.relearn.removeItem=(e,t)=>e.removeItem(t),window.T_Copy_to_clipboard=`Copy text to clipboard`,window.T_Copied_to_clipboard=`Text copied to clipboard!`,window.T_Link_copied_to_clipboard=`Link copied to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`,window.T_Browser_unsupported_feature=`This browser doesn't support this feature`,window.relearn.themevariants=["blue"],window.relearn.customvariantprefix="my-custom-",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.relearn.setItem(window.localStorage,window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&(document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}})),window.relearn.markVariant())},window.relearn.markVariant=function(){var e=window.relearn.getItem(window.localStorage,window.relearn.absBaseUri+"/variant");document.querySelectorAll(".R-variantswitcher select").forEach(t=>{t.value=e})},window.relearn.initVariant=function(){var e=window.relearn.getItem(window.localStorage,window.relearn.absBaseUri+"/variant")??"";(!e||!e.startsWith(window.relearn.customvariantprefix)&&!window.relearn.themevariants.includes(e)||e.startsWith(window.relearn.customvariantprefix)&&!window.relearn.getItem(window.localStorage,window.relearn.absBaseUri+"/variantstylesheet-"+e))&&(e=window.relearn.themevariants[0],window.relearn.setItem(window.localStorage,window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant()</script></head><body class="mobile-support print" data-url=/lectures/lecture7/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar class=default-animation><div class="topbar-wrapper default-animation"><div class="topbar-sidebar-divider default-animation"></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar default-animation" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><span class="btn cstyle button link noborder notitle interactive"><button onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></span></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/index.html><span itemprop=name>Introduction to Machine Learning 1DL034</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/lectures/index.html><span itemprop=name>Lectures</span></a><meta itemprop=position content="2">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>Lecture 7</span><meta itemprop=position content="3"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-print default-animation" data-content-empty=disable data-width-s=area-more data-width-m=show data-width-l=show><span class="btn cstyle button link noborder notitle interactive"><a href=/lectures/lecture7/index.print.html title="Print whole chapter (CTRL+ALT+p)"><i class="fa-fw fas fa-print"></i></a></span></div><div class="topbar-button topbar-button-more default-animation" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><span class="btn cstyle button link noborder notitle interactive"><button onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button></span><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable lectures" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=lecture-7-clustering-and-nearest-neighbours>Lecture 7: Clustering and Nearest Neighbours</h1><h2 id=todays-topics>Today&rsquo;s Topics<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type=button title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span></h2><p>In this segment you are looking at some unsupervised algorithms, as
well as one supervised learning method (K-nearest neighbours) The
main unsupervised algorithms are Hierarchical clustering, K-means
clustering and DBSCAN. It is important not to get K-nearest neighbours
and K-means clustering confused.</p><p>The K-means algorithm works by gradient descent. Unlike a lot of the
algorithms that we have been looking at, K-means often suffers from
the problem of many local minima. In Andrew Ng&rsquo;s lectures you will
meet various ways of dealing with local minima.</p><p>If you have taken <a href=https://ad2-uu-se.github.io/ rel=external>Algorithms and Data Structures II</a>(<a href="https://www.uu.se/en/admissions/master/selma/kursplan/?kKod=1DL231" rel=external>1DL231</a>)
or AD3
(<a href="https://www.uu.se/en/admissions/master/selma/kursplan/?kKod=1DL481" rel=external>1DL481</a>),
then you will have met the concept of <a href=https://ad2-uu-se.github.io/lectures/lectures12-13/index.html rel=external>NP-hardness</a>. K-means clustering is NP-hard
(see the reference below). This means that the problem is not easy to
solve. If you could guarantee that there would only be one global
minimum then gradient descent would be an efficient algorithm. This
implies that there will always often be local minima in K-means
clustering.</p><h2 id=slides>Slides<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type=button title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span></h2><p>I used these
<a href=http://user.it.uu.se/~justin/Assets/Teaching/IntroML/Slides/lecture7.pdf rel=external>slides</a>
in the lecture.</p><h2 id=reading-guide>Reading Guide<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type=button title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span></h2><h3 id=k-means-clustering>K-Means Clustering<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type=button title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span></h3><ul><li><a href=http://themlbook.com/ rel=external>Hundred-Page Machine Learning Book</a> <a href="https://www.dropbox.com/s/qiq2c85cle9ydb6/Chapter3.pdf?dl=0" rel=external>Chapter
3</a>
section 3.5 and <a href="https://www.dropbox.com/s/y9a7b0hzmuksqar/Chapter9.pdf?dl=0" rel=external>Chapter
9</a> all
of 9.2</li><li>Chapter 7 of <a href=https://www.cambridge.org/highereducation/books/a-hands-on-introduction-to-machine-learning/3E57313A963BF7AF5C6330EB88ADAB2E#overview rel=external>A Hands-On Introduction to Machine
Learning</a>.</li><li>Chapter 6 (6.1 and 6.2) of <a href=https://tinyurl.com/y2obtldw rel=external>A First Course in Machine
Learning</a>. The link takes you to the
electronic copy in the library.</li><li>This is only for reference: <a href=https://cseweb.ucsd.edu/~avattani/papers/kmeans_hardness.pdf rel=external>NP Hardness of K-means
clustering</a>. You
don&rsquo;t need to understand the proof, although you should be aware of
its implications. No greedy/gradient descent algorithm for K-means
is going to be exact.</li></ul><h3 id=k-nearest-neighbours>K-Nearest Neighbours<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type=button title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span></h3><ul><li>The <a href=https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm rel=external>Wikipedia page on K-Nearest
Neighbours</a>
is a good starting point.</li></ul><h2 id=what-should-i-know-by-the-end-of-this-lecture>What should I know by the end of this lecture?<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type=button title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span></h2><ul><li>What are some of the applications of clustering?</li><li>What is hierarchical clustering and what algorithms are there?</li><li>How does the K-means algorithm work? What is the cost function?</li><li>What is a local optima and why is it a problem with the K-means
algorithm?</li><li>What are some approaches to choosing the number of clusters in
K-means?</li><li>How does the K-nearest neighbour algorithm work and what are some of
its applications?</li><li>What is DBSCAN and how does it work?</li></ul><footer class=footline></footer></article></div></main></div><script src=/js/perfect-scrollbar/perfect-scrollbar.min.js?1769686469 defer></script><script src=/js/theme.min.js?1769686469 defer></script><div id=toast-container role=status aria-live=polite aria-atomic=false></div></body></html>